from flask import Flask, render_template, request, send_file, redirect, url_for, jsonify
from flask_socketio import SocketIO, emit
import os
import subprocess
import whisper
import torch
try:
    import openai
except ImportError:
    openai = None
import requests
import json
from datetime import datetime
import fitz
from fitz import Document
import pytesseract
from PIL import Image
import pandas as pd
import tempfile
from typing import cast, Optional, Dict, List, Tuple, Any
import logging
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
import uuid
import threading
import time
import re
from functools import wraps
import psutil
import base64
from io import BytesIO
from openai.types.chat import ChatCompletionMessageParam
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
app.config['UPLOAD_FOLDER'] = tempfile.mkdtemp()
socketio = SocketIO(app, cors_allowed_origins="*")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'mistral')

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
OPENAI_MODELS = [
    {'id': 'gpt-4o-mini', 'name': 'GPT-4o Mini (æ¨è)', 'provider': 'openai'},
    {'id': 'gpt-4o', 'name': 'GPT-4o', 'provider': 'openai'},
    {'id': 'gpt-4-turbo', 'name': 'GPT-4 Turbo', 'provider': 'openai'},
    {'id': 'gpt-3.5-turbo', 'name': 'GPT-3.5 Turbo', 'provider': 'openai'},
    # OpenRouter models
    {'id': 'deepseek/deepseek-chat', 'name': 'DeepSeek Chat', 'provider': 'openrouter'},
    {'id': 'anthropic/claude-3-haiku', 'name': 'Claude 3 Haiku', 'provider': 'openrouter'},
    {'id': 'meta-llama/llama-3.1-70b-instruct', 'name': 'Llama 3.1 70B', 'provider': 'openrouter'},
    {'id': 'mistralai/mistral-7b-instruct', 'name': 'Mistral 7B Instruct', 'provider': 'openrouter'},
    {'id': 'google/gemini-pro', 'name': 'Gemini Pro', 'provider': 'openrouter'},
]

OLLAMA_DEFAULT_MODELS = [
    {'id': 'mistral', 'name': 'Mistral (æ¨è)', 'size': '7B'},
    {'id': 'deepseek-coder', 'name': 'DeepSeek Coder', 'size': '7B'},
    {'id': 'llama3.1', 'name': 'Llama 3.1', 'size': '8B'},
    {'id': 'qwen2', 'name': 'Qwen2', 'size': '7B'},
    {'id': 'gemma2', 'name': 'Gemma 2', 'size': '9B'},
]

# æ£€æŸ¥GPUå¯ç”¨æ€§
def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒMPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'
        # æ£€æŸ¥CUDA
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    except Exception as e:
        logger.warning(f"GPUæ£€æŸ¥å¤±è´¥: {e}")
        return 'cpu'

# åˆå§‹åŒ–Whisperæ¨¡å‹
def initialize_whisper_model():
    """åˆå§‹åŒ–Whisperæ¨¡å‹"""
    try:
        device = check_gpu_availability()
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–Whisperæ¨¡å‹... ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device == 'mps':
            # Apple Silicon MPS
            model = whisper.load_model('medium', device='mps')
        elif device == 'cuda':
            # NVIDIA GPU
            model = whisper.load_model('medium', device='cuda')
        else:
            # CPU
            model = whisper.load_model('medium', device='cpu')
            
        logger.info(f"Whisperæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {device}")
        return model, device
    except Exception as e:
        logger.error(f"Whisperæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        # é™çº§åˆ°CPU
        model = whisper.load_model('medium', device='cpu')
        return model, 'cpu'

whisper_model, whisper_device = initialize_whisper_model()

# é”™è¯¯å¤„ç†è£…é¥°å™¨
def handle_errors(max_retries=3):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.error(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        raise e
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            return None
        return wrapper
    return decorator

# è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
@handle_errors(max_retries=2)
def get_available_ollama_models():
    """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹"""
    try:
        logger.info(f"æ­£åœ¨æ£€æµ‹Ollamaæ¨¡å‹ï¼ŒURL: {OLLAMA_BASE_URL}")
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            available_models = []
            
            if 'models' in data and data['models']:
                # è·å–å·²å®‰è£…çš„æ¨¡å‹åç§°
                installed_names = set()
                for model in data['models']:
                    model_name = model['name'].split(':')[0]
                    installed_names.add(model_name)
                    logger.info(f"æ£€æµ‹åˆ°å·²å®‰è£…çš„Ollamaæ¨¡å‹: {model_name}")
                
                # åŒ¹é…é»˜è®¤æ¨¡å‹åˆ—è¡¨
                for default_model in OLLAMA_DEFAULT_MODELS:
                    if default_model['id'] in installed_names:
                        available_models.append(default_model)
                        logger.info(f"æ·»åŠ å¯ç”¨æ¨¡å‹: {default_model['name']}")
                
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°é»˜è®¤æ¨¡å‹ï¼Œæ·»åŠ æ‰€æœ‰å·²å®‰è£…çš„æ¨¡å‹
                if not available_models:
                    for model in data['models']:
                        model_name = model['name'].split(':')[0]
                        available_models.append({
                            'id': model_name,
                            'name': model_name.title(),
                            'size': 'Unknown'
                        })
                        logger.info(f"æ·»åŠ æ£€æµ‹åˆ°çš„æ¨¡å‹: {model_name}")
            
            logger.info(f"å…±æ£€æµ‹åˆ° {len(available_models)} ä¸ªå¯ç”¨Ollamaæ¨¡å‹")
            return available_models
        else:
            logger.warning(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: HTTP {response.status_code}")
            return []
    except requests.exceptions.ConnectionError:
        logger.warning("æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ")
        return []
    except Exception as e:
        logger.error(f"è·å–Ollamaæ¨¡å‹å¤±è´¥: {e}")
        return []

class ProcessingProgress:
    def __init__(self):
        self.steps = []
        self.current_step = 0
        self.total_steps = 0
        self.session_id: Optional[str] = None
        self.start_time: Optional[float] = None
        self.estimated_total_time: Optional[float] = None
        
    def add_step(self, step_name: str, description: str = "", estimated_duration: float = 0):
        self.steps.append({
            'name': step_name,
            'description': description,
            'status': 'pending',
            'start_time': None,
            'end_time': None,
            'estimated_duration': estimated_duration,
            'progress_percentage': 0
        })
        self.total_steps = len(self.steps)
        
    def start_step(self, step_index: int):
        if 0 <= step_index < len(self.steps):
            self.current_step = step_index
            self.steps[step_index]['status'] = 'processing'
            self.steps[step_index]['start_time'] = time.time()
            
            if self.start_time is None:
                self.start_time = time.time()
                
            step_name = self.steps[step_index]['name']
            logger.info(f"ğŸš€ [{self.session_id[:8] if self.session_id else 'LOCAL'}] å¼€å§‹: {step_name}")
            self.emit_progress()
            
    def update_step_progress(self, step_index: int, progress: float):
        """æ›´æ–°æ­¥éª¤å†…éƒ¨è¿›åº¦"""
        if 0 <= step_index < len(self.steps):
            self.steps[step_index]['progress_percentage'] = min(100, max(0, progress))
            self.emit_progress()
            
    def complete_step(self, step_index: int):
        if 0 <= step_index < len(self.steps):
            self.steps[step_index]['status'] = 'completed'
            self.steps[step_index]['end_time'] = time.time()
            self.steps[step_index]['progress_percentage'] = 100
            
            step_name = self.steps[step_index]['name']
            duration = self.steps[step_index]['end_time'] - self.steps[step_index]['start_time']
            logger.info(f"âœ… [{self.session_id[:8] if self.session_id else 'LOCAL'}] å®Œæˆ: {step_name} (è€—æ—¶ {duration:.1f}ç§’)")
            self.emit_progress()
            
    def estimate_remaining_time(self):
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if not self.start_time:
            return None
            
        elapsed_time = time.time() - self.start_time
        completed_steps = sum(1 for step in self.steps if step['status'] == 'completed')
        
        if completed_steps == 0:
            return None
            
        avg_time_per_step = elapsed_time / completed_steps
        remaining_steps = self.total_steps - completed_steps
        
        return remaining_steps * avg_time_per_step
            
    def emit_progress(self):
        if self.session_id:
            # è®¡ç®—æ€»ä½“è¿›åº¦
            total_progress = 0
            for i, step in enumerate(self.steps):
                if step['status'] == 'completed':
                    total_progress += 100
                elif step['status'] == 'processing':
                    total_progress += step['progress_percentage']
            
            overall_percentage = total_progress / self.total_steps if self.total_steps > 0 else 0
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´
            remaining_time = self.estimate_remaining_time()
            
            # å¤„ç†datetimeåºåˆ—åŒ–é—®é¢˜
            serialized_steps = []
            for step in self.steps:
                serialized_step = {
                    'name': step['name'],
                    'description': step['description'],
                    'status': step['status'],
                    'progress_percentage': step['progress_percentage'],
                    'start_time': step['start_time'],
                    'end_time': step['end_time']
                }
                serialized_steps.append(serialized_step)
            
            progress_data = {
                'current_step': self.current_step,
                'total_steps': self.total_steps,
                'steps': serialized_steps,
                'percentage': overall_percentage,
                'estimated_remaining_time': remaining_time
            }
            
            # å‘½ä»¤è¡Œç›‘æ§è¾“å‡º
            current_step_name = self.steps[self.current_step]['name'] if self.current_step < len(self.steps) else "å®Œæˆ"
            remaining_str = f", é¢„è®¡å‰©ä½™ {remaining_time:.1f}ç§’" if remaining_time else ""
            logger.info(f"ğŸ”„ [{self.session_id[:8]}] è¿›åº¦: {overall_percentage:.1f}% - {current_step_name}{remaining_str}")
            
            socketio.emit('progress_update', progress_data, to=self.session_id)

@handle_errors(max_retries=3)
def call_openai_api(prompt: str, model: Optional[str] = None, max_tokens: int = 2000, image_base64: Optional[str] = None) -> Optional[str]:
    """è°ƒç”¨OpenAI APIæˆ–OpenRouter APIï¼Œæ”¯æŒå¤šæ¨¡æ€"""
    try:
        if not openai:
            logger.warning("ğŸš« OpenAI library not available")
            return None
            
        if not OPENAI_API_KEY:
            logger.warning("ğŸ”‘ OPENAI_API_KEY not configured in .env file")
            return None
            
        used_model = model or OPENAI_MODEL
        logger.info(f"ğŸ¤– è°ƒç”¨OpenAI API - æ¨¡å‹: {used_model}, åŸºç¡€URL: {OPENAI_BASE_URL}")
        
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )

        messages: List[ChatCompletionMessageParam] = [
            {"role": "system", "content": "You are a professional meeting assistant specialized in transcription correction and meeting summary generation. Please respond in Chinese."},
        ]

        if image_base64:
            # å¤šæ¨¡æ€è°ƒç”¨
            used_model = 'gpt-4o' # å¼ºåˆ¶ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹
            messages.append({
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}"
                        }
                    }
                ]
            })
        else:
            # çº¯æ–‡æœ¬è°ƒç”¨
            messages.append({"role": "user", "content": prompt})

        response = client.chat.completions.create(
            model=used_model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        content = response.choices[0].message.content
        result = content.strip() if content else ""
        logger.info(f"âœ… OpenAI API è°ƒç”¨æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
        return result
        
    except Exception as e:
        logger.error(f"âŒ OpenAI API call failed: {str(e)}")
        return None

@handle_errors(max_retries=3)
def call_ollama_api(prompt: str, model: Optional[str] = None, max_tokens: int = 2000) -> Optional[str]:
    """è°ƒç”¨æœ¬åœ°ollamaæ¨¡å‹"""
    try:
        used_model = model or OLLAMA_MODEL
        logger.info(f"ğŸ¦™ è°ƒç”¨Ollama API - æ¨¡å‹: {used_model}, è¾“å…¥é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": used_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.7
                }
            },
            timeout=300
        )
        
        if response.status_code == 200:
            result = response.json().get('response', '').strip()
            logger.info(f"âœ… Ollama API è°ƒç”¨æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
            return result
        else:
            logger.error(f"âŒ Ollama API call failed with status {response.status_code}")
            return None
            
    except Exception as e:
        logger.error(f"âŒ Ollama API call failed: {str(e)}")
        return None

@handle_errors(max_retries=3)
def advanced_transcription_correction(transcript: str, context: str, reference_docs: List[str], progress: ProcessingProgress, step_index: int, ai_provider: str, model: str) -> str:
    """Corrects transcript using AI, context, and reference docs."""
    progress.start_step(step_index)
    reference_content = "\n\n---\n\n".join(reference_docs)
    prompt = correction_prompt_template.replace("{{transcript}}", transcript) \
                                       .replace("{{context}}", context) \
                                       .replace("{{reference_docs}}", reference_content)
    progress.update_step_progress(step_index, 30)
    corrected_text = None
    if ai_provider == 'openai' and OPENAI_API_KEY:
        corrected_text = call_openai_api(prompt, model=model, max_tokens=4000)
    elif ai_provider == 'ollama':
        corrected_text = call_ollama_api(prompt, model=model, max_tokens=4000)
    progress.update_step_progress(step_index, 80)
    if not corrected_text:
        logger.warning("AI correction failed, returning original transcript.")
        corrected_text = transcript
    progress.complete_step(step_index)
    return corrected_text

@handle_errors(max_retries=3)
def generate_meeting_summary(corrected_transcript: str, context: str, reference_docs: List[str], progress: ProcessingProgress, step_index: int, ai_provider: str, model: str) -> Dict[str, str]:
    """Generates a meeting summary using AI."""
    progress.start_step(step_index)
    reference_content = "\n\n---\n\n".join(reference_docs)
    prompt = summary_prompt_template.replace("{{corrected_transcript}}", corrected_transcript) \
                                    .replace("{{context}}", context) \
                                    .replace("{{reference_docs}}", reference_content)
    progress.update_step_progress(step_index, 30)
    summary = None
    if ai_provider == 'openai' and OPENAI_API_KEY:
        summary = call_openai_api(prompt, model=model, max_tokens=4000)
    elif ai_provider == 'ollama':
        summary = call_ollama_api(prompt, model=model, max_tokens=4000)
    progress.update_step_progress(step_index, 80)
    if not summary:
        logger.warning("AI summary generation failed, creating a basic summary.")
        summary = f"# Meeting Summary\n\n## Key Points\n- AI summary generation failed. This is a fallback summary.\n\n## Full Transcript\n{corrected_transcript}"
    progress.complete_step(step_index)
    return {"summary": summary}

def extract_text_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            doc = fitz.open(file_path)
            text = ""
            for page_num in range(len(doc)):
                page = doc[page_num]
                text += page.get_text()  # type: ignore
            doc.close()
            return text
        elif file_ext in ['.md', '.txt']:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            logger.warning(f"Unsupported file type: {file_ext}")
            return ""
    except Exception as e:
        logger.error(f"Error extracting text from {file_path}: {str(e)}")
        return ""

def estimate_audio_duration(audio_path: str) -> float:
    """ä¼°ç®—éŸ³é¢‘æ—¶é•¿"""
    try:
        result = subprocess.run(
            ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', audio_path],
            capture_output=True, text=True
        )
        return float(result.stdout.strip())
    except Exception as e:
        logger.error(f"æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿: {e}")
        return 0

@app.route('/')
def index():
    # è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
    available_ollama_models = get_available_ollama_models()
    
    # æ£€æŸ¥é…ç½®çŠ¶æ€
    config_status = {
        'openai_configured': bool(OPENAI_API_KEY),
        'ollama_available': len(available_ollama_models) > 0,
        'whisper_device': whisper_device
    }
    
    return render_template('index.html', 
                         openai_models=OPENAI_MODELS,
                         ollama_models=available_ollama_models,
                         config_status=config_status,
                         whisper_device=whisper_device)

@app.route('/get_models')
def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    available_ollama_models = get_available_ollama_models()
    config_status = {
        'openai_configured': bool(OPENAI_API_KEY),
        'ollama_available': len(available_ollama_models) > 0
    }
    
    return jsonify({
        'openai_models': OPENAI_MODELS,
        'ollama_models': available_ollama_models,
        'config_status': config_status,
        'whisper_device': whisper_device
    })

@socketio.on('connect')
def handle_connect():
    logger.info(f"Client connected: {request.sid}")  # type: ignore

@socketio.on('disconnect')
def handle_disconnect():
    logger.info(f"Client disconnected: {request.sid}")  # type: ignore

@socketio.on('join')
def handle_join(session_id):
    """å®¢æˆ·ç«¯åŠ å…¥ä¼šè¯æˆ¿é—´"""
    from flask_socketio import join_room
    join_room(session_id)
    logger.info(f"Client {request.sid} joined session: {session_id}")  # type: ignore
    emit('joined', {'session_id': session_id})

@app.route('/process', methods=['POST'])
def process_meeting():
    session_id = str(uuid.uuid4())
    try:
        video = request.files.get('video')
        docs = request.files.getlist('docs[]')
        context_input = request.form.get('context', '')
        ai_provider = request.form.get('aiProvider', 'openai')
        
        # Determine the model to use
        if ai_provider == 'openai':
            model = request.form.get('model', OPENAI_MODELS[0]['id'])
        else:
            # Fallback for ollama if no model is selected or available
            available_ollama = get_available_ollama_models()
            model = request.form.get('model', available_ollama[0]['id'] if available_ollama else OLLAMA_DEFAULT_MODELS[0]['id'])

        logger.info(f"æ”¶åˆ°æ–°çš„å¤„ç†è¯·æ±‚ - Session: {session_id[:8]}, AI: {ai_provider}, Model: {model}")
        if video and video.filename:
            logger.info(f"  - è§†é¢‘æ–‡ä»¶: {video.filename}")
        
        # å¢å¼ºæ—¥å¿—ï¼Œè®°å½•æ‰€æœ‰æ”¶åˆ°çš„æ–‡æ¡£æ–‡ä»¶å
        if docs:
            file_names = [doc.filename for doc in docs if doc.filename]
            logger.info(f"  - å‚è€ƒæ–‡æ¡£: {len(file_names)} ä¸ª -> {file_names}")

        if not video or not video.filename:
            return jsonify({"error": "è¯·é€‰æ‹©è§†é¢‘æ–‡ä»¶"}), 400
        
        if ai_provider == 'openai' and not OPENAI_API_KEY:
            return jsonify({"error": "OpenAI API Keyæœªåœ¨æœåŠ¡å™¨ç«¯é…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"}), 400
        
        if ai_provider == 'ollama':
            available_models = get_available_ollama_models()
            if not available_models:
                return jsonify({"error": "æœªæ£€æµ‹åˆ°å¯ç”¨çš„Ollamaæ¨¡å‹ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ"}), 400
        
        # ä¿å­˜è§†é¢‘æ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶åå®‰å…¨
        video_filename = secure_filename(video.filename) if video and video.filename else f"video-{session_id}.mov"
        mov_path = os.path.join(app.config['UPLOAD_FOLDER'], video_filename)
        video.save(mov_path)
        
        # å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶åå”¯ä¸€
        doc_files = []
        for doc in docs:
            if doc and doc.filename:
                original_filename = secure_filename(doc.filename)
                # åˆ›å»ºå”¯ä¸€æ–‡ä»¶åæ¥é¿å…è¦†ç›–
                unique_filename = f"{session_id[:8]}-{uuid.uuid4().hex[:8]}-{original_filename}"
                saved_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
                doc.save(saved_path)
                doc_files.append({'original_filename': original_filename, 'saved_path': saved_path})
        
        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¤„ç†
        thread = threading.Thread(
            target=process_meeting_async, 
            args=(session_id, mov_path, video_filename, doc_files, context_input, ai_provider, model)
        )
        thread.start()
        
        return jsonify({"session_id": session_id, "status": "processing"})
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚é”™è¯¯: {str(e)}")
        return jsonify({"error": f"å¤„ç†è¯·æ±‚é”™è¯¯: {str(e)}"}), 500

def process_meeting_async(session_id: str, mov_path: str, video_filename: str, doc_files: list, context_input: str, ai_provider: str, model: str):
    """Asynchronously processes the meeting from video to summary."""
    progress = ProcessingProgress()
    progress.session_id = session_id
    logger.info(f"ğŸ¬ [{session_id[:8]}] Starting meeting processing...")
    start_time = time.time()
    
    try:
        # Define processing steps
        progress.add_step("video_validation", "Validating video file", 2)
        progress.add_step("audio_extraction", "Extracting audio", 10)
        progress.add_step("speech_transcription", "Transcribing audio to text", 60)
        progress.add_step("document_processing", "Processing reference documents", 5)
        progress.add_step("image_analysis", "Analyzing images in PDFs", 20)
        progress.add_step("ai_correction", "Correcting transcript with AI", 30)
        progress.add_step("summary_generation", "Generating meeting summary", 25)
        progress.add_step("file_generation", "Creating downloadable files", 5)

        # Step 1: Validate Video
        progress.start_step(0)
        base_name = os.path.splitext(video_filename)[0]
        if not os.path.exists(mov_path):
            raise FileNotFoundError(f"Video file not found: {mov_path}")
        progress.complete_step(0)

        # Step 2: Extract Audio
        progress.start_step(1)
        audio_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_audio.wav')
        subprocess.run(['ffmpeg', '-i', mov_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', audio_path, '-y'], check=True, capture_output=True, text=True)
        audio_duration = estimate_audio_duration(audio_path)
        progress.complete_step(1)

        # Step 3: Transcribe Speech
        progress.start_step(2)
        result = whisper_model.transcribe(audio_path, language=None)
        transcript = str(result["text"])
        detected_lang = result.get("language", "unknown")
        progress.complete_step(2)

        # Step 4: Process Reference Documents
        progress.start_step(3)
        doc_texts = []
        pdf_files_for_image_analysis = []
        total_docs = len(doc_files)
        for i, doc_info in enumerate(doc_files):
            original_filename, saved_path = doc_info['original_filename'], doc_info['saved_path']
            progress.steps[3]['description'] = f"Processing {i+1}/{total_docs}: {original_filename}"
            progress.emit_progress()
            
            text = extract_text_from_file(saved_path)
            if text:
                doc_texts.append(f"## Reference: {original_filename}\n\n{text}")
            if original_filename.lower().endswith('.pdf'):
                pdf_files_for_image_analysis.append(saved_path)
        progress.complete_step(3)

        # Step 5: Analyze Images in PDFs
        progress.start_step(4)
        if ai_provider == 'openai':
            image_analysis_results = []
            for pdf_path in pdf_files_for_image_analysis:
                image_analysis_results.extend(analyze_pdf_images(pdf_path, ai_provider))
            if image_analysis_results:
                doc_texts.extend(image_analysis_results)
        progress.complete_step(4)

        # Step 6: AI Correction
        corrected_transcript = advanced_transcription_correction(transcript, context_input, doc_texts, progress, 5, ai_provider, model)
        
        # Step 7: Generate Summary
        meeting_summary = generate_meeting_summary(corrected_transcript, context_input, doc_texts, progress, 6, ai_provider, model)
        
        # Step 8: Generate Files
        progress.start_step(7)
        
        base_name = os.path.splitext(video_filename)[0]
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Generate raw transcript file
        raw_transcript_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_raw_transcript.md')
        with open(raw_transcript_path, 'w', encoding='utf-8') as f:
            f.write(f'# åŸå§‹è¯­éŸ³è½¬å½•\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {now_str}\n')
            f.write(f'**å¤„ç†è®¾å¤‡**: {whisper_device.upper()}\n')
            f.write(f'**æ£€æµ‹è¯­è¨€**: {detected_lang}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n\n')
            f.write(f'## è½¬å½•å†…å®¹\n\n{transcript}\n')

        # Generate corrected transcript file
        corrected_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_corrected_transcript.md')
        with open(corrected_path, 'w', encoding='utf-8') as f:
            f.write(f'# AIä¿®æ­£åçš„ä¼šè®®è½¬å½•\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {now_str}\n')
            f.write(f'**AIæ¨¡å‹**: {model}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n\n')
            f.write(f'## ä¿®æ­£åçš„è½¬å½•å†…å®¹\n\n{corrected_transcript}\n')

        # Generate full meeting report
        report_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_meeting_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f'# ä¼šè®®çºªè¦æŠ¥å‘Š\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {now_str}\n')
            f.write(f'**AIæ¨¡å‹**: {model}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n')
            f.write(f'**å‚è€ƒèµ„æ–™**: {len(doc_texts)} ä¸ª\n\n')
            
            if doc_texts:
                f.write(f'## å‚è€ƒèµ„æ–™æ¦‚è§ˆ\n\n')
                for i, doc in enumerate(doc_texts, 1):
                    first_line = doc.split('\n')[0] if doc else 'æœªçŸ¥æ–‡æ¡£'
                    f.write(f'{i}. {first_line.replace("##", "").strip()}\n')
                f.write(f'\n---\n\n')
            
            f.write(f'{meeting_summary["summary"]}\n\n')
            f.write(f'---\n\n## é™„å½•ï¼šAIä¿®æ­£åå®Œæ•´è®°å½•\n\n{corrected_transcript}\n')

        progress.complete_step(7)

        # Final step: Notify client
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ [{session_id[:8]}] Processing complete! Total time: {total_time:.1f}s")
        
        socketio.emit('processing_complete', {
            'raw_transcript_path': raw_transcript_path,
            'corrected_transcript_path': corrected_path,
            'report_path': report_path,
            'processing_time': total_time,
            'audio_duration': audio_duration,
            'detected_language': detected_lang,
            'ai_provider': ai_provider,
            'model': model
        }, to=session_id)

    except Exception as e:
        logger.error(f"ğŸ’¥ [{session_id[:8]}] Processing error: {e}", exc_info=True)
        socketio.emit('error', {'message': f'An error occurred: {str(e)}'}, to=session_id)

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶"""
    try:
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if os.path.exists(file_path):
            return send_file(file_path, as_attachment=True)
        else:
            return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½é”™è¯¯: {e}")
        return jsonify({"error": f"æ–‡ä»¶ä¸‹è½½é”™è¯¯: {str(e)}"}), 500

def analyze_pdf_images(file_path: str, ai_provider: str) -> List[str]:
    """ä»PDFä¸­æå–å›¾ç‰‡å¹¶ä½¿ç”¨AIè¿›è¡Œåˆ†æ"""
    if ai_provider != 'openai' or not OPENAI_API_KEY:
        logger.info("éOpenAIæä¾›å•†æˆ–æœªé…ç½®Keyï¼Œè·³è¿‡å›¾ç‰‡åˆ†æ")
        return []

    logger.info(f"ğŸ–¼ï¸ å¼€å§‹ä»PDFä¸­æå–å’Œåˆ†æå›¾ç‰‡: {os.path.basename(file_path)}")
    image_analyses = []
    try:
        doc = fitz.open(file_path)
        for page_num in range(len(doc)):
            image_list = doc.get_page_images(page_num)
            for img_index, img in enumerate(image_list, 1):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # å°†å›¾ç‰‡è½¬æ¢ä¸ºå¯å‘é€çš„æ ¼å¼
                image = Image.open(BytesIO(image_bytes))
                
                # ç¡®ä¿æ˜¯RGBæ ¼å¼
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                    
                buffered = BytesIO()
                image.save(buffered, format="JPEG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                
                # AIåˆ†æå›¾ç‰‡
                prompt = "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ªå›¾è¡¨ï¼Œè¯·è§£è¯»å…¶æ•°æ®ã€è¶‹åŠ¿å’Œå…³é”®ä¿¡æ¯ã€‚å¦‚æœæ˜¯ä¸€ä¸ªæµç¨‹å›¾ï¼Œè¯·è§£é‡Šå…¶æ­¥éª¤å’Œé€»è¾‘ã€‚å¦‚æœæ˜¯ä¸€å¼ æˆªå›¾ï¼Œè¯·æè¿°å…¶ç•Œé¢å’ŒåŠŸèƒ½ã€‚"
                analysis = call_openai_api(prompt, image_base64=img_base64, max_tokens=500)
                
                if analysis:
                    analysis_text = f"## æ¥è‡ªPDF '{os.path.basename(file_path)}' (ç¬¬{page_num+1}é¡µ, å›¾{img_index})çš„å›¾è¡¨åˆ†æ\n\n{analysis}\n"
                    image_analyses.append(analysis_text)
                    logger.info(f"âœ… æˆåŠŸåˆ†æäº†PDF '{os.path.basename(file_path)}' ä¸­çš„ä¸€å¼ å›¾ç‰‡")
                else:
                    logger.warning(f"âš ï¸ æœªèƒ½åˆ†æPDF '{os.path.basename(file_path)}' ä¸­çš„ä¸€å¼ å›¾ç‰‡")

        doc.close()
    except Exception as e:
        logger.error(f"âŒ åˆ†æPDFå›¾ç‰‡æ—¶å‡ºé”™: {e}")
    
    return image_analyses

# ä»æ–‡ä»¶åŠ è½½æç¤ºè¯
def load_prompt(filename: str) -> str:
    """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯"""
    try:
        with open(os.path.join('prompts', filename), 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        logger.error(f"æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°: {filename}")
        return ""

correction_prompt_template = load_prompt('correction_prompt.txt')
summary_prompt_template = load_prompt('summary_prompt.txt')

if __name__ == '__main__':
    # å¯åŠ¨æ—¶æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logger.info("=" * 50)
    logger.info("ğŸ¤ æ™ºèƒ½ä¼šè®®çºªè¦ç”Ÿæˆå™¨å¯åŠ¨")
    logger.info(f"ğŸ–¥ï¸  Whisperè®¾å¤‡: {whisper_device.upper()}")
    logger.info(f"ğŸ¤– OpenAIé…ç½®: {'âœ… å·²é…ç½®' if OPENAI_API_KEY else 'âŒ æœªé…ç½®'}")
    
    # æ£€æŸ¥Ollama
    ollama_models = get_available_ollama_models()
    logger.info(f"ğŸ¦™ Ollamaæ¨¡å‹: {len(ollama_models)} ä¸ªå¯ç”¨")
    for model in ollama_models:
        logger.info(f"   - {model['name']}")
    
    # æ£€æŸ¥æç¤ºè¯æ–‡ä»¶
    logger.info("=" * 50)
    logger.info("ğŸ“ åŠ è½½AIæç¤ºè¯:")
    logger.info(f"   - è½¬å½•ä¿®æ­£: {'âœ… å·²åŠ è½½' if correction_prompt_template else 'âŒ åŠ è½½å¤±è´¥'}")
    logger.info(f"   - çºªè¦ç”Ÿæˆ: {'âœ… å·²åŠ è½½' if summary_prompt_template else 'âŒ åŠ è½½å¤±è´¥'}")
    logger.info("=" * 50)
    
    socketio.run(app, debug=True, port=5000)