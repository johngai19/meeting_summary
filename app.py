from flask import Flask, render_template, request, send_file, redirect, url_for, jsonify
from flask_socketio import SocketIO, emit
import os
import subprocess
import whisper
import torch
try:
    import openai
except ImportError:
    openai = None
import requests
import json
from datetime import datetime
import fitz
from fitz import Document
import pytesseract
from PIL import Image
import pandas as pd
import tempfile
from typing import cast, Optional, Dict, List, Tuple
import logging
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
import uuid
import threading
import time
import re
from functools import wraps
import psutil

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
app.config['UPLOAD_FOLDER'] = tempfile.mkdtemp()
socketio = SocketIO(app, cors_allowed_origins="*")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'mistral')

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
OPENAI_MODELS = [
    {'id': 'gpt-4o-mini', 'name': 'GPT-4o Mini (æ¨è)', 'provider': 'openai'},
    {'id': 'gpt-4o', 'name': 'GPT-4o', 'provider': 'openai'},
    {'id': 'gpt-4-turbo', 'name': 'GPT-4 Turbo', 'provider': 'openai'},
    {'id': 'gpt-3.5-turbo', 'name': 'GPT-3.5 Turbo', 'provider': 'openai'},
    # OpenRouter models
    {'id': 'deepseek/deepseek-chat', 'name': 'DeepSeek Chat', 'provider': 'openrouter'},
    {'id': 'anthropic/claude-3-haiku', 'name': 'Claude 3 Haiku', 'provider': 'openrouter'},
    {'id': 'meta-llama/llama-3.1-70b-instruct', 'name': 'Llama 3.1 70B', 'provider': 'openrouter'},
    {'id': 'mistralai/mistral-7b-instruct', 'name': 'Mistral 7B Instruct', 'provider': 'openrouter'},
    {'id': 'google/gemini-pro', 'name': 'Gemini Pro', 'provider': 'openrouter'},
]

OLLAMA_DEFAULT_MODELS = [
    {'id': 'mistral', 'name': 'Mistral (æ¨è)', 'size': '7B'},
    {'id': 'deepseek-coder', 'name': 'DeepSeek Coder', 'size': '7B'},
    {'id': 'llama3.1', 'name': 'Llama 3.1', 'size': '8B'},
    {'id': 'qwen2', 'name': 'Qwen2', 'size': '7B'},
    {'id': 'gemma2', 'name': 'Gemma 2', 'size': '9B'},
]

# æ£€æŸ¥GPUå¯ç”¨æ€§
def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒMPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'
        # æ£€æŸ¥CUDA
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    except Exception as e:
        logger.warning(f"GPUæ£€æŸ¥å¤±è´¥: {e}")
        return 'cpu'

# åˆå§‹åŒ–Whisperæ¨¡å‹
def initialize_whisper_model():
    """åˆå§‹åŒ–Whisperæ¨¡å‹"""
    try:
        device = check_gpu_availability()
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–Whisperæ¨¡å‹... ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device == 'mps':
            # Apple Silicon MPS
            model = whisper.load_model('medium', device='mps')
        elif device == 'cuda':
            # NVIDIA GPU
            model = whisper.load_model('medium', device='cuda')
        else:
            # CPU
            model = whisper.load_model('medium', device='cpu')
            
        logger.info(f"Whisperæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {device}")
        return model, device
    except Exception as e:
        logger.error(f"Whisperæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        # é™çº§åˆ°CPU
        model = whisper.load_model('medium', device='cpu')
        return model, 'cpu'

whisper_model, whisper_device = initialize_whisper_model()

# é”™è¯¯å¤„ç†è£…é¥°å™¨
def handle_errors(max_retries=3):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.error(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        raise e
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            return None
        return wrapper
    return decorator

# è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
@handle_errors(max_retries=2)
def get_available_ollama_models():
    """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹"""
    try:
        logger.info(f"æ­£åœ¨æ£€æµ‹Ollamaæ¨¡å‹ï¼ŒURL: {OLLAMA_BASE_URL}")
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            available_models = []
            
            if 'models' in data and data['models']:
                # è·å–å·²å®‰è£…çš„æ¨¡å‹åç§°
                installed_names = set()
                for model in data['models']:
                    model_name = model['name'].split(':')[0]
                    installed_names.add(model_name)
                    logger.info(f"æ£€æµ‹åˆ°å·²å®‰è£…çš„Ollamaæ¨¡å‹: {model_name}")
                
                # åŒ¹é…é»˜è®¤æ¨¡å‹åˆ—è¡¨
                for default_model in OLLAMA_DEFAULT_MODELS:
                    if default_model['id'] in installed_names:
                        available_models.append(default_model)
                        logger.info(f"æ·»åŠ å¯ç”¨æ¨¡å‹: {default_model['name']}")
                
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°é»˜è®¤æ¨¡å‹ï¼Œæ·»åŠ æ‰€æœ‰å·²å®‰è£…çš„æ¨¡å‹
                if not available_models:
                    for model in data['models']:
                        model_name = model['name'].split(':')[0]
                        available_models.append({
                            'id': model_name,
                            'name': model_name.title(),
                            'size': 'Unknown'
                        })
                        logger.info(f"æ·»åŠ æ£€æµ‹åˆ°çš„æ¨¡å‹: {model_name}")
            
            logger.info(f"å…±æ£€æµ‹åˆ° {len(available_models)} ä¸ªå¯ç”¨Ollamaæ¨¡å‹")
            return available_models
        else:
            logger.warning(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: HTTP {response.status_code}")
            return []
    except requests.exceptions.ConnectionError:
        logger.warning("æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ")
        return []
    except Exception as e:
        logger.error(f"è·å–Ollamaæ¨¡å‹å¤±è´¥: {e}")
        return []

class ProcessingProgress:
    def __init__(self):
        self.steps = []
        self.current_step = 0
        self.total_steps = 0
        self.session_id: Optional[str] = None
        self.start_time: Optional[float] = None
        self.estimated_total_time: Optional[float] = None
        
    def add_step(self, step_name: str, description: str = "", estimated_duration: float = 0):
        self.steps.append({
            'name': step_name,
            'description': description,
            'status': 'pending',
            'start_time': None,
            'end_time': None,
            'estimated_duration': estimated_duration,
            'progress_percentage': 0
        })
        self.total_steps = len(self.steps)
        
    def start_step(self, step_index: int):
        if 0 <= step_index < len(self.steps):
            self.current_step = step_index
            self.steps[step_index]['status'] = 'processing'
            self.steps[step_index]['start_time'] = time.time()
            
            if self.start_time is None:
                self.start_time = time.time()
                
            step_name = self.steps[step_index]['name']
            logger.info(f"ğŸš€ [{self.session_id[:8] if self.session_id else 'LOCAL'}] å¼€å§‹: {step_name}")
            self.emit_progress()
            
    def update_step_progress(self, step_index: int, progress: float):
        """æ›´æ–°æ­¥éª¤å†…éƒ¨è¿›åº¦"""
        if 0 <= step_index < len(self.steps):
            self.steps[step_index]['progress_percentage'] = min(100, max(0, progress))
            self.emit_progress()
            
    def complete_step(self, step_index: int):
        if 0 <= step_index < len(self.steps):
            self.steps[step_index]['status'] = 'completed'
            self.steps[step_index]['end_time'] = time.time()
            self.steps[step_index]['progress_percentage'] = 100
            
            step_name = self.steps[step_index]['name']
            duration = self.steps[step_index]['end_time'] - self.steps[step_index]['start_time']
            logger.info(f"âœ… [{self.session_id[:8] if self.session_id else 'LOCAL'}] å®Œæˆ: {step_name} (è€—æ—¶ {duration:.1f}ç§’)")
            self.emit_progress()
            
    def estimate_remaining_time(self):
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if not self.start_time:
            return None
            
        elapsed_time = time.time() - self.start_time
        completed_steps = sum(1 for step in self.steps if step['status'] == 'completed')
        
        if completed_steps == 0:
            return None
            
        avg_time_per_step = elapsed_time / completed_steps
        remaining_steps = self.total_steps - completed_steps
        
        return remaining_steps * avg_time_per_step
            
    def emit_progress(self):
        if self.session_id:
            # è®¡ç®—æ€»ä½“è¿›åº¦
            total_progress = 0
            for i, step in enumerate(self.steps):
                if step['status'] == 'completed':
                    total_progress += 100
                elif step['status'] == 'processing':
                    total_progress += step['progress_percentage']
            
            overall_percentage = total_progress / self.total_steps if self.total_steps > 0 else 0
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´
            remaining_time = self.estimate_remaining_time()
            
            # å¤„ç†datetimeåºåˆ—åŒ–é—®é¢˜
            serialized_steps = []
            for step in self.steps:
                serialized_step = {
                    'name': step['name'],
                    'description': step['description'],
                    'status': step['status'],
                    'progress_percentage': step['progress_percentage'],
                    'start_time': step['start_time'],
                    'end_time': step['end_time']
                }
                serialized_steps.append(serialized_step)
            
            progress_data = {
                'current_step': self.current_step,
                'total_steps': self.total_steps,
                'steps': serialized_steps,
                'percentage': overall_percentage,
                'estimated_remaining_time': remaining_time
            }
            
            # å‘½ä»¤è¡Œç›‘æ§è¾“å‡º
            current_step_name = self.steps[self.current_step]['name'] if self.current_step < len(self.steps) else "å®Œæˆ"
            remaining_str = f", é¢„è®¡å‰©ä½™ {remaining_time:.1f}ç§’" if remaining_time else ""
            logger.info(f"ğŸ”„ [{self.session_id[:8]}] è¿›åº¦: {overall_percentage:.1f}% - {current_step_name}{remaining_str}")
            
            socketio.emit('progress_update', progress_data, to=self.session_id)

@handle_errors(max_retries=3)
def call_openai_api(prompt: str, model: Optional[str] = None, max_tokens: int = 2000) -> Optional[str]:
    """è°ƒç”¨OpenAI APIæˆ–OpenRouter API"""
    try:
        if not openai:
            logger.warning("ğŸš« OpenAI library not available")
            return None
            
        if not OPENAI_API_KEY:
            logger.warning("ğŸ”‘ OPENAI_API_KEY not configured in .env file")
            return None
            
        used_model = model or OPENAI_MODEL
        logger.info(f"ğŸ¤– è°ƒç”¨OpenAI API - æ¨¡å‹: {used_model}, åŸºç¡€URL: {OPENAI_BASE_URL}")
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )
        
        response = client.chat.completions.create(
            model=used_model,
            messages=[
                {"role": "system", "content": "You are a professional meeting assistant specialized in transcription correction and meeting summary generation. Please respond in Chinese."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        content = response.choices[0].message.content
        result = content.strip() if content else ""
        logger.info(f"âœ… OpenAI API è°ƒç”¨æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
        return result
        
    except Exception as e:
        logger.error(f"âŒ OpenAI API call failed: {str(e)}")
        return None

@handle_errors(max_retries=3)
def call_ollama_api(prompt: str, model: Optional[str] = None, max_tokens: int = 2000) -> Optional[str]:
    """è°ƒç”¨æœ¬åœ°ollamaæ¨¡å‹"""
    try:
        used_model = model or OLLAMA_MODEL
        logger.info(f"ğŸ¦™ è°ƒç”¨Ollama API - æ¨¡å‹: {used_model}, è¾“å…¥é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": used_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.7
                }
            },
            timeout=300
        )
        
        if response.status_code == 200:
            result = response.json().get('response', '').strip()
            logger.info(f"âœ… Ollama API è°ƒç”¨æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
            return result
        else:
            logger.error(f"âŒ Ollama API call failed with status {response.status_code}")
            return None
            
    except Exception as e:
        logger.error(f"âŒ Ollama API call failed: {str(e)}")
        return None

def advanced_transcription_correction(transcript: str, reference_docs: List[str], progress: ProcessingProgress, step_index: int, ai_provider: str) -> str:
    """é«˜çº§è½¬å½•æ–‡æœ¬çº é”™"""
    progress.start_step(step_index)
    
    try:
        # å¤„ç†å‚è€ƒæ–‡æ¡£ï¼Œåˆ†ç±»æ˜¾ç¤º
        reference_content = ""
        if reference_docs:
            reference_content = f"""
## å‚è€ƒèƒŒæ™¯èµ„æ–™ï¼š
æœ¬æ¬¡ä¼šè®®æä¾›äº†ä»¥ä¸‹å‚è€ƒèµ„æ–™ï¼Œè¯·åœ¨ä¿®æ­£è¿‡ç¨‹ä¸­å‚è€ƒè¿™äº›ä¿¡æ¯ï¼š

"""
            for i, doc in enumerate(reference_docs, 1):
                doc_preview = doc[:800] + "..." if len(doc) > 800 else doc
                reference_content += f"""
### å‚è€ƒèµ„æ–™ {i}ï¼š
{doc_preview}

"""
        
        # æ„å»ºä¸“ä¸šçš„æç¤ºè¯
        correction_prompt = f"""
## è§’è‰²å®šä½
ä½ æ˜¯ä¸€å**ä¸“ä¸šçš„ä¼šè®®è®°å½•ä¿®è®¢ä¸“å®¶**ï¼Œè´Ÿè´£å°†è¯­éŸ³è½¬å†™çš„åŸå§‹æ–‡æœ¬æ¸…ç†ã€æ¶¦è‰²å¹¶ç»“æ„åŒ–æˆå¯è¯»æ€§å¼ºã€é€»è¾‘æ¸…æ™°çš„ä¼šè®®çºªè¦ã€‚

---

## è¾“å…¥å˜é‡
- `transcript`ï¼šåŸå§‹è¯­éŸ³è½¬æ–‡å­—è¯†åˆ«ç»“æœï¼ˆçº¯æ–‡æœ¬ï¼‰
- `reference_docs`ï¼šä¸ä¼šè®®ä¸»é¢˜ç›¸å…³çš„å‚è€ƒèƒŒæ™¯èµ„æ–™

---

## ä¿®æ­£ä¸ä¼˜åŒ–æ­¥éª¤

### 1. è¯­è¨€æ¸…ç†
- **å»é™¤è¯­æ°”è¯**ï¼šå¦‚"å—¯"ã€"å•Š"ã€"å‘ƒ"ã€"é‚£ä¸ª"ã€"è¿™ä¸ª"ã€"å°±æ˜¯è¯´"ã€"ç„¶åå‘¢"ç­‰
- **åˆ é™¤é‡å¤ä¸å†—ä½™**ï¼šå»æ‰é‡å¤è¯è¯­ã€åºŸè¯ã€å¡é¡¿æˆ–è¯†åˆ«å™ªéŸ³
- **æ¶ˆé™¤è¯†åˆ«é”™è¯¯**ï¼šä¿®æ­£åŒéŸ³å­—ã€è¿‘éŸ³å­—åŠä¹±ç 

### 2. è¯­æ³•ä¸è¡¨è¾¾
- **æ ¡æ­£è¯­æ³•é”™è¯¯**ï¼šè°ƒæ•´ä¸»è°“å®¾ã€æ ‡ç‚¹ä½¿ç”¨ã€æ–­å¥ä½ç½®
- **æå‡ä¸“ä¸šæ€§**ï¼šä¸°å¯Œè¯æ±‡ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾ï¼Œå¥å¼ç®€æ´å‡ç»ƒ
- **ç»Ÿä¸€æ—¶æ€ä¸è¯­æ€**ï¼šä¿æŒå…¨æ–‡ä¸€è‡´

### 3. å†…å®¹ç»“æ„åŒ–
- **åˆ†æ®µä¸æ ‡é¢˜**ï¼šæ ¹æ®è®®é¢˜æˆ–å‘è¨€äººåˆ†æ®µï¼Œå¹¶ä¸ºæ¯ä¸€éƒ¨åˆ†æ·»åŠ åˆé€‚çš„äºŒçº§æ ‡é¢˜
- **æ·»åŠ åˆ—è¡¨ä¸è¦ç‚¹**ï¼šå°†é‡è¦å†³ç­–ã€è¡ŒåŠ¨é¡¹ã€é—®é¢˜ä¸ç»“è®ºï¼Œç”¨æ— åºæˆ–æœ‰åºåˆ—è¡¨å‘ˆç°
- **æ ‡æ³¨å…³é”®ä¿¡æ¯**ï¼šå¦‚"å†³ç­–"ã€"å¾…åŠäº‹é¡¹"ã€"é—®é¢˜"ã€"è´Ÿè´£äºº"ç­‰ï¼Œç”¨åŠ ç²—çªå‡º

### 4. ä¸“ä¸šæœ¯è¯­æ ‡å‡†åŒ–
- **å¯¹ç…§å‚è€ƒèµ„æ–™**ï¼šå°†æœ¯è¯­ä¸å‚è€ƒæ–‡æ¡£ä¸­çš„æ ‡å‡†ä¿æŒä¸€è‡´ï¼Œä¿®æ­£é”™è¯¯ç”¨è¯
- **ä¿æŒä¸€è‡´æ€§**ï¼šç¡®ä¿åŒä¸€æ¦‚å¿µåœ¨å…¨æ–‡ä¸­ä½¿ç”¨ç»Ÿä¸€çš„æœ¯è¯­è¡¨è¾¾

### 5. ä¸Šä¸‹æ–‡ä¸é€»è¾‘
- **è¿è´¯æ€§æ£€æŸ¥**ï¼šç¡®ä¿å‰åé€»è¾‘æµç•…ï¼Œå¿…è¦æ—¶è¡¥å……è¿‡æ¸¡è¯­å¥
- **èƒŒæ™¯è¡¥å……**ï¼šç»“åˆå‚è€ƒèµ„æ–™ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œå¯¹å®¹æ˜“è¯¯è§£æˆ–é—æ¼çš„éƒ¨åˆ†è¿›è¡Œæ³¨é‡Šæˆ–è¡¥å……

---

{reference_content}

## åŸå§‹è¯­éŸ³è½¬æ–‡å­—å†…å®¹ï¼š
{transcript}

---

## è¾“å‡ºè¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºä¿®æ­£åçš„å†…å®¹ï¼š

```markdown
# ä¼šè®®å†…å®¹ï¼ˆä¿®æ­£ç‰ˆï¼‰

## ä¸»è¦è®¨è®ºå†…å®¹
### è®®é¢˜ä¸€ï¼š[æ ¹æ®å®é™…å†…å®¹ç¡®å®š]
- **å‘è¨€è¦ç‚¹**ï¼š
  1. ...
  2. ...

### è®®é¢˜äºŒï¼š[æ ¹æ®å®é™…å†…å®¹ç¡®å®š]
- **å…³é”®å†³ç­–**ï¼š...
- **è¡ŒåŠ¨é¡¹ç›®**ï¼š...

## é‡è¦ä¿¡æ¯æ±‡æ€»
- **å†³ç­–äº‹é¡¹**ï¼š...
- **å¾…åŠä»»åŠ¡**ï¼š...
- **å…³é”®æ•°æ®**ï¼š...
```

è¯·æä¾›ä¿®æ­£åçš„ä¼šè®®å†…å®¹ï¼š
"""
        
        progress.update_step_progress(step_index, 30)
        
        # æ ¹æ®AIæä¾›å•†è°ƒç”¨ç›¸åº”çš„API
        corrected_text = None
        if ai_provider == 'openai' and OPENAI_API_KEY:
            corrected_text = call_openai_api(correction_prompt, max_tokens=4000)
        elif ai_provider == 'ollama':
            corrected_text = call_ollama_api(correction_prompt, max_tokens=4000)
        
        progress.update_step_progress(step_index, 80)
        
        if not corrected_text:
            logger.warning("âš ï¸ AIçº é”™å¤±è´¥ï¼Œè¿”å›åŸå§‹è½¬å½•æ–‡æœ¬")
            corrected_text = transcript
        
        progress.complete_step(step_index)
        return corrected_text
        
    except Exception as e:
        logger.error(f"âŒ è½¬å½•çº é”™å¤±è´¥: {str(e)}")
        progress.complete_step(step_index)
        return transcript

def generate_meeting_summary(corrected_transcript: str, reference_docs: List[str], progress: ProcessingProgress, step_index: int, ai_provider: str) -> Dict[str, str]:
    """ç”Ÿæˆè¯¦ç»†çš„ä¼šè®®çºªè¦"""
    progress.start_step(step_index)
    
    try:
        # æ„å»ºå‚è€ƒæ–‡æ¡£å±•ç¤º
        reference_content = ""
        if reference_docs:
            reference_content = f"""
## å‚è€ƒèƒŒæ™¯èµ„æ–™ï¼š
æœ¬æ¬¡ä¼šè®®åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™è¿›è¡Œè®¨è®ºï¼Œè¯·åœ¨ç”Ÿæˆçºªè¦æ—¶ä½“ç°è¿™äº›èƒŒæ™¯ä¿¡æ¯çš„å½±å“ï¼š

"""
            for i, doc in enumerate(reference_docs, 1):
                doc_preview = doc[:600] + "..." if len(doc) > 600 else doc
                reference_content += f"""
### å‚è€ƒèµ„æ–™ {i}ï¼š
{doc_preview}

"""
        
        summary_prompt = f"""
## è§’è‰²å®šä½
ä½ æ˜¯ä¸€å**èµ„æ·±çš„ä¼šè®®çºªè¦ä¸“å®¶**ï¼Œè´Ÿè´£å°†ä¿®æ­£åçš„ä¼šè®®è½¬å½•å†…å®¹è½¬åŒ–ä¸ºä¸“ä¸šã€ç»“æ„åŒ–ã€å…·æœ‰å®ç”¨ä»·å€¼çš„ä¼šè®®çºªè¦ã€‚

---

## è¾“å…¥å˜é‡
- `corrected_transcript`ï¼šå·²ä¿®æ­£çš„ä¼šè®®è½¬å½•å†…å®¹
- `reference_docs`ï¼šä¸ä¼šè®®ä¸»é¢˜ç›¸å…³çš„å‚è€ƒèƒŒæ™¯èµ„æ–™

---

## åˆ†æä¸ç”Ÿæˆæ­¥éª¤

### 1. å†…å®¹æ·±åº¦åˆ†æ
- **è¯†åˆ«æ ¸å¿ƒä¸»é¢˜**ï¼šç¡®å®šä¼šè®®çš„ä¸»è¦ç›®æ ‡å’Œè®¨è®ºé‡ç‚¹
- **åˆ†æè®¨è®ºå±‚æ¬¡**ï¼šç†è§£è®®é¢˜çš„é€»è¾‘å…³ç³»å’Œé‡è¦æ€§æ’åº
- **æå–å…³é”®è§‚ç‚¹**ï¼šè¯†åˆ«é‡è¦æ„è§ã€äº‰è®®ç‚¹å’Œè¾¾æˆçš„å…±è¯†
- **å‘ç°éšå«ä¿¡æ¯**ï¼šæ³¨æ„æš—ç¤ºçš„é—®é¢˜ã€æœºä¼šå’Œæœªæ˜ç¡®è¡¨è¾¾çš„éœ€æ±‚

### 2. ä¿¡æ¯æå–ä¸æ•´ç†
- **äº‹å®æ•°æ®æ”¶é›†**ï¼šæå–å…·ä½“çš„æ•°å­—ã€æ—¶é—´ã€åœ°ç‚¹ã€äººå‘˜ä¿¡æ¯
- **å†³ç­–è¯†åˆ«**ï¼šåŒºåˆ†å·²ç¡®å®šçš„å†³ç­–å’Œå¾…å®šçš„å€¾å‘
- **è¡ŒåŠ¨é¡¹æ¢³ç†**ï¼šæ•´ç†å…·ä½“çš„ä»»åŠ¡ã€è´£ä»»äººå’Œæ—¶é—´è¦æ±‚
- **ä¼˜å…ˆçº§åˆ¤æ–­**ï¼šæŒ‰é‡è¦æ€§å’Œç´§æ€¥æ€§å¯¹å†…å®¹è¿›è¡Œæ’åº

### 3. ç»“æ„åŒ–å¤„ç†
- **é€»è¾‘ç»„ç»‡**ï¼šæŒ‰ç…§è®®é¢˜ç›¸å…³æ€§å’Œé‡è¦æ€§é‡æ–°ç»„ç»‡å†…å®¹
- **åˆ†ç±»æ ‡æ³¨**ï¼šæ˜ç¡®åŒºåˆ†å†³ç­–ã€è¡ŒåŠ¨é¡¹ã€é—®é¢˜ã€ç»“è®ºç­‰ä¸åŒç±»å‹ä¿¡æ¯
- **å±‚æ¬¡å»ºç«‹**ï¼šåˆ›å»ºæ¸…æ™°çš„ä¿¡æ¯å±‚æ¬¡ç»“æ„

---

{reference_content}

## ä¼šè®®è½¬å½•å†…å®¹ï¼ˆå·²ä¿®æ­£ï¼‰ï¼š
{corrected_transcript}

---

## è¾“å‡ºæ ¼å¼è¦æ±‚
è¯·æŒ‰ç…§ä»¥ä¸‹ä¸“ä¸šæ ¼å¼ç”Ÿæˆä¼šè®®çºªè¦ï¼š

```markdown
# ä¼šè®®çºªè¦

## 1. ä¼šè®®èƒŒæ™¯
> åŸºäºå®é™…å†…å®¹åˆ†æçš„ä¼šè®®ç›®çš„ã€èƒŒæ™¯å’Œå‚ä¸æƒ…å†µ

## 2. ä¸»è¦è®¨è®ºå†…å®¹
### 2.1 è®®é¢˜ä¸€ï¼š[æ ¹æ®å®é™…å†…å®¹ç¡®å®šæ ‡é¢˜]
- **å‘è¨€è¦ç‚¹**ï¼š
  1. ...
  2. ...
- **å…³é”®è§‚ç‚¹**ï¼š...
- **äº‰è®®ç‚¹**ï¼š...

### 2.2 è®®é¢˜äºŒï¼š[æ ¹æ®å®é™…å†…å®¹ç¡®å®šæ ‡é¢˜]
- **è®¨è®ºè¦ç‚¹**ï¼š...
- **æŠ€æœ¯ç»†èŠ‚**ï¼š...

## 3. å†³ç­–ä¸å…±è¯†
| å†³ç­–é¡¹ | å…·ä½“å†…å®¹ | å½±å“èŒƒå›´ |
| ------ | -------- | -------- |
| ...    | ...      | ...      |

## 4. è¡ŒåŠ¨è®¡åˆ’
| è¡ŒåŠ¨é¡¹ | è´Ÿè´£äºº | å®Œæˆæ—¶é—´ | ä¼˜å…ˆçº§ |
| ------ | ------ | -------- | ------ |
| ...    | ...    | ...      | ...    |

## 5. é‡è¦ä¿¡æ¯æ±‡æ€»
- **å…³é”®æ•°æ®**ï¼š...
- **æ—¶é—´èŠ‚ç‚¹**ï¼š...
- **ç›¸å…³äººå‘˜**ï¼š...
- **æŠ€æœ¯è¦ç‚¹**ï¼š...

## 6. å¾…è§£å†³é—®é¢˜
1. ...
2. ...

## 7. è·Ÿè¿›äº‹é¡¹
- **ä¸‹æ¬¡ä¼šè®®**ï¼š...
- **éœ€è¦ç¡®è®¤**ï¼š...
- **åç»­æ²Ÿé€š**ï¼š...

## 8. æœ¯è¯­è¯´æ˜ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰
| æœ¯è¯­ | å«ä¹‰ |
| ---- | ---- |
| ...  | ...  |

## 9. é™„å½•
- **å‚è€ƒæ–‡æ¡£**ï¼šåŸºäºæä¾›çš„å‚è€ƒèµ„æ–™
- **ä¼šè®®æ—¶é•¿**ï¼š...
- **å‚ä¸äººæ•°**ï¼š...
```

---

## ç‰¹åˆ«è¦æ±‚
1. **é¿å…ç®€å•å¤åˆ¶**ï¼šä¸è¦ç›´æ¥å¤åˆ¶è½¬å½•å†…å®¹ï¼Œè€Œæ˜¯åŸºäºç†è§£è¿›è¡Œæç‚¼å’Œé‡ç»„
2. **çªå‡ºå®ç”¨ä»·å€¼**ï¼šé‡ç‚¹å…³æ³¨ä¼šè®®çš„å®é™…æˆæœã€å†³ç­–å’Œè¡ŒåŠ¨è®¡åˆ’
3. **ä½“ç°èƒŒæ™¯å…³è”**ï¼šå¦‚æœæœ‰å‚è€ƒèµ„æ–™ï¼Œè¦ä½“ç°å…¶ä¸ä¼šè®®è®¨è®ºçš„å…³è”æ€§
4. **ä¿æŒä¸“ä¸šæ€§**ï¼šä½¿ç”¨å•†åŠ¡è¯­è¨€ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾
5. **ç¡®ä¿é€»è¾‘æ€§**ï¼šå†…å®¹ç»„ç»‡è¦æœ‰æ¸…æ™°çš„é€»è¾‘ç»“æ„
6. **æ³¨é‡å¯æ“ä½œæ€§**ï¼šè¡ŒåŠ¨é¡¹è¦å…·ä½“æ˜ç¡®ï¼Œä¾¿äºåç»­æ‰§è¡Œ

è¯·ç”Ÿæˆä¸“ä¸šçš„ä¼šè®®çºªè¦ï¼š
"""
        
        progress.update_step_progress(step_index, 30)
        
        # æ ¹æ®AIæä¾›å•†è°ƒç”¨ç›¸åº”çš„API
        summary = None
        if ai_provider == 'openai' and OPENAI_API_KEY:
            summary = call_openai_api(summary_prompt, max_tokens=4000)
        elif ai_provider == 'ollama':
            summary = call_ollama_api(summary_prompt, max_tokens=4000)
        
        progress.update_step_progress(step_index, 80)
        
        if not summary:
            logger.warning("âš ï¸ AIä¼šè®®çºªè¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†ææ ¼å¼")
            # æ™ºèƒ½åˆ†æè½¬å½•å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
            lines = corrected_transcript.split('\n')
            content_lines = [line.strip() for line in lines if line.strip()]
            
            # ç®€å•çš„å…³é”®è¯æå–
            keywords = []
            for line in content_lines:
                if any(keyword in line for keyword in ['å†³å®š', 'ç¡®å®š', 'è®¡åˆ’', 'å®‰æ’', 'è´Ÿè´£', 'å®Œæˆ']):
                    keywords.append(line)
            
            summary = f"""## ä¼šè®®æ¦‚è¦
æœ¬æ¬¡ä¼šè®®ä¸»è¦å›´ç»•ç›¸å…³è®®é¢˜è¿›è¡Œäº†æ·±å…¥è®¨è®ºï¼Œå½¢æˆäº†å¤šé¡¹é‡è¦å…±è¯†ã€‚

## ä¸»è¦è®¨è®ºå†…å®¹
{chr(10).join(content_lines[:10])}
{'...' if len(content_lines) > 10 else ''}

## é‡è¦å†³è®®ä¸è¡ŒåŠ¨è®¡åˆ’
{chr(10).join(keywords[:5]) if keywords else 'ä¼šè®®è®¨è®ºäº†ç›¸å…³è®®é¢˜ï¼Œå½¢æˆäº†åˆæ­¥å…±è¯†ã€‚'}

## å®Œæ•´è½¬å½•å†…å®¹
{corrected_transcript}

---
_æ³¨ï¼šæ­¤çºªè¦ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œå»ºè®®é…ç½®AIæœåŠ¡è·å¾—æ›´ä¸“ä¸šçš„ä¼šè®®çºªè¦åˆ†æã€‚_"""
        
        progress.complete_step(step_index)
        return {"summary": summary}
        
    except Exception as e:
        logger.error(f"âŒ ä¼šè®®çºªè¦ç”Ÿæˆå¤±è´¥: {str(e)}")
        progress.complete_step(step_index)
        return {"summary": f"ä¼šè®®çºªè¦ç”Ÿæˆå¤±è´¥: {str(e)}"}

def extract_text_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            doc = fitz.open(file_path)
            text = ""
            for page_num in range(len(doc)):
                page = doc[page_num]
                text += page.get_text()  # type: ignore
            doc.close()
            return text
        elif file_ext in ['.md', '.txt']:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            logger.warning(f"Unsupported file type: {file_ext}")
            return ""
    except Exception as e:
        logger.error(f"Error extracting text from {file_path}: {str(e)}")
        return ""

def estimate_audio_duration(audio_path: str) -> float:
    """ä¼°ç®—éŸ³é¢‘æ—¶é•¿"""
    try:
        result = subprocess.run(
            ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', audio_path],
            capture_output=True, text=True
        )
        return float(result.stdout.strip())
    except Exception as e:
        logger.error(f"æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿: {e}")
        return 0

@app.route('/')
def index():
    # è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
    available_ollama_models = get_available_ollama_models()
    
    # æ£€æŸ¥é…ç½®çŠ¶æ€
    config_status = {
        'openai_configured': bool(OPENAI_API_KEY),
        'ollama_available': len(available_ollama_models) > 0,
        'whisper_device': whisper_device
    }
    
    return render_template('index.html', 
                         openai_models=OPENAI_MODELS,
                         ollama_models=available_ollama_models,
                         config_status=config_status,
                         whisper_device=whisper_device)

@app.route('/get_models')
def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    available_ollama_models = get_available_ollama_models()
    config_status = {
        'openai_configured': bool(OPENAI_API_KEY),
        'ollama_available': len(available_ollama_models) > 0
    }
    
    return jsonify({
        'openai_models': OPENAI_MODELS,
        'ollama_models': available_ollama_models,
        'config_status': config_status,
        'whisper_device': whisper_device
    })

@socketio.on('connect')
def handle_connect():
    logger.info(f"Client connected: {request.sid}")  # type: ignore

@socketio.on('disconnect')
def handle_disconnect():
    logger.info(f"Client disconnected: {request.sid}")  # type: ignore

@socketio.on('join')
def handle_join(session_id):
    """å®¢æˆ·ç«¯åŠ å…¥ä¼šè¯æˆ¿é—´"""
    from flask_socketio import join_room
    join_room(session_id)
    logger.info(f"Client {request.sid} joined session: {session_id}")  # type: ignore
    emit('joined', {'session_id': session_id})

@app.route('/process', methods=['POST'])
def process_meeting():
    session_id = str(uuid.uuid4())
    
    try:
        # è·å–è¡¨å•æ•°æ®
        video = request.files.get('video')
        docs = request.files.getlist('docs')
        text_input = request.form.get('docsText', '')
        ai_provider = request.form.get('aiProvider', 'openai')
        
        if not video or not video.filename:
            return jsonify({"error": "è¯·é€‰æ‹©è§†é¢‘æ–‡ä»¶"}), 400
        
        # æ£€æŸ¥AIæœåŠ¡é…ç½®
        if ai_provider == 'openai' and not OPENAI_API_KEY:
            return jsonify({"error": "OpenAI API Keyæœªåœ¨æœåŠ¡å™¨ç«¯é…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"}), 400
        
        if ai_provider == 'ollama':
            available_models = get_available_ollama_models()
            if not available_models:
                return jsonify({"error": "æœªæ£€æµ‹åˆ°å¯ç”¨çš„Ollamaæ¨¡å‹ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ"}), 400
        
        # åœ¨ä¸»çº¿ç¨‹ä¸­ä¿å­˜æ–‡ä»¶
        video_filename = video.filename
        base_name = os.path.splitext(video_filename)[0]
        mov_path = os.path.join(app.config['UPLOAD_FOLDER'], video_filename)
        video.save(mov_path)
        
        # å¤„ç†æ–‡æ¡£æ–‡ä»¶
        doc_files = []
        for doc in docs:
            if doc and doc.filename:
                doc_path = os.path.join(app.config['UPLOAD_FOLDER'], doc.filename)
                doc.save(doc_path)
                doc_files.append({'filename': doc.filename, 'path': doc_path})
        
        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¤„ç†
        thread = threading.Thread(
            target=process_meeting_async, 
            args=(session_id, mov_path, video_filename, doc_files, text_input, ai_provider)
        )
        thread.start()
        
        return jsonify({"session_id": session_id, "status": "processing"})
    
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚é”™è¯¯: {str(e)}")
        return jsonify({"error": f"å¤„ç†è¯·æ±‚é”™è¯¯: {str(e)}"}), 500

def process_meeting_async(session_id: str, mov_path: str, video_filename: str, doc_files: list, text_input: str, ai_provider: str):
    """å¼‚æ­¥å¤„ç†ä¼šè®®"""
    progress = ProcessingProgress()
    progress.session_id = session_id
    
    logger.info(f"ğŸ¬ [{session_id[:8]}] å¼€å§‹å¤„ç†ä¼šè®® - è§†é¢‘: {video_filename}, AI: {ai_provider}")
    start_time = time.time()
    
    try:
        # è®¾ç½®å¤„ç†æ­¥éª¤ï¼ˆå¸¦æ—¶é—´ä¼°ç®—ï¼‰
        progress.add_step("video_validation", "éªŒè¯è§†é¢‘æ–‡ä»¶", 2)
        progress.add_step("audio_extraction", "æå–éŸ³é¢‘", 10)
        progress.add_step("speech_transcription", "è¯­éŸ³è½¬æ–‡å­—", 60)
        progress.add_step("document_processing", "å¤„ç†å‚è€ƒæ–‡æ¡£", 5)
        progress.add_step("ai_correction", "AIæ™ºèƒ½çº é”™", 30)
        progress.add_step("summary_generation", "ç”Ÿæˆä¼šè®®çºªè¦", 25)
        progress.add_step("file_generation", "ç”Ÿæˆä¸‹è½½æ–‡ä»¶", 5)
        
        # æ­¥éª¤1ï¼šéªŒè¯è§†é¢‘æ–‡ä»¶
        progress.start_step(0)
        base_name = os.path.splitext(video_filename)[0]
        
        if not os.path.exists(mov_path):
            logger.error(f"âŒ [{session_id[:8]}] è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {mov_path}")
            socketio.emit('error', {'message': 'è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨'}, to=session_id)
            return
            
        file_size = os.path.getsize(mov_path)
        logger.info(f"ğŸ“ [{session_id[:8]}] è§†é¢‘æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f} MB")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        memory_usage = psutil.virtual_memory().percent
        if memory_usage > 85:
            logger.warning(f"âš ï¸ ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜: {memory_usage}%")
            
        progress.complete_step(0)
        
        # æ­¥éª¤2ï¼šæå–éŸ³é¢‘
        progress.start_step(1)
        audio_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_audio.wav')
        
        try:
            subprocess.run([
                'ffmpeg', '-i', mov_path, '-vn', '-acodec', 'pcm_s16le', 
                '-ar', '16000', '-ac', '1', audio_path, '-y'
            ], check=True, capture_output=True, text=True)
            
            audio_duration = estimate_audio_duration(audio_path)
            logger.info(f"ğŸµ [{session_id[:8]}] éŸ³é¢‘æå–å®Œæˆ - æ—¶é•¿: {audio_duration:.1f}ç§’")
            
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
            socketio.emit('error', {'message': 'éŸ³é¢‘æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ ¼å¼'}, to=session_id)
            return
            
        progress.complete_step(1)
        
        # æ­¥éª¤3ï¼šè¯­éŸ³è½¬æ–‡å­—
        progress.start_step(2)
        
        try:
            logger.info(f"ğŸ¤ [{session_id[:8]}] å¼€å§‹è¯­éŸ³è½¬å½• - è®¾å¤‡: {whisper_device}")
            
            # æ›´æ–°è¿›åº¦
            for i in range(0, 91, 10):
                progress.update_step_progress(2, i)
                time.sleep(0.1)
            
            result = whisper_model.transcribe(audio_path, language=None)
            transcript = str(result["text"])
            
            detected_lang = result.get("language", "unknown")
            logger.info(f"ğŸ¤ [{session_id[:8]}] è½¬å½•å®Œæˆ - è¯­è¨€: {detected_lang}, å­—ç¬¦æ•°: {len(transcript)}")
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³è½¬å½•å¤±è´¥: {e}")
            socketio.emit('error', {'message': f'è¯­éŸ³è½¬å½•å¤±è´¥: {str(e)}'}, to=session_id)
            return
            
        progress.complete_step(2)
        
        # æ­¥éª¤4ï¼šå¤„ç†å‚è€ƒæ–‡æ¡£
        progress.start_step(3)
        doc_texts = []
        
        # å¤„ç†æ–‡æœ¬è¾“å…¥ï¼ˆä½œä¸ºèƒŒæ™¯ä¿¡æ¯ï¼‰
        if text_input and text_input.strip():
            formatted_text = f"## ä¼šè®®èƒŒæ™¯ä¿¡æ¯\n\n{text_input.strip()}"
            doc_texts.append(formatted_text)
            logger.info(f"ğŸ“ [{session_id[:8]}] æ·»åŠ èƒŒæ™¯ä¿¡æ¯: {len(text_input.strip())} å­—ç¬¦")
        
        # å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£æ–‡ä»¶
        for doc_info in doc_files:
            try:
                text = extract_text_from_file(doc_info['path'])
                if text:
                    # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ æ ‡è¯†
                    file_ext = os.path.splitext(doc_info['filename'])[1].lower()
                    doc_type = {
                        '.pdf': 'PDFæ–‡æ¡£',
                        '.doc': 'Wordæ–‡æ¡£',
                        '.docx': 'Wordæ–‡æ¡£',
                        '.md': 'Markdownæ–‡æ¡£',
                        '.txt': 'æ–‡æœ¬æ–‡æ¡£'
                    }.get(file_ext, 'æ–‡æ¡£')
                    
                    formatted_text = f"## å‚è€ƒæ–‡æ¡£ï¼š{doc_info['filename']} ({doc_type})\n\n{text}"
                    doc_texts.append(formatted_text)
                    logger.info(f"ğŸ“„ [{session_id[:8]}] å¤„ç†{doc_type}: {doc_info['filename']}, æå– {len(text)} å­—ç¬¦")
            except Exception as e:
                logger.warning(f"âš ï¸ [{session_id[:8]}] æ–‡æ¡£å¤„ç†å¤±è´¥: {doc_info['filename']}, é”™è¯¯: {str(e)}")
        
        logger.info(f"ğŸ“š [{session_id[:8]}] å‚è€ƒæ–‡æ¡£å¤„ç†å®Œæˆ - å…± {len(doc_texts)} ä¸ªèµ„æ–™")
        progress.complete_step(3)
        
        # æ­¥éª¤5ï¼šAIæ™ºèƒ½çº é”™
        corrected_transcript = advanced_transcription_correction(transcript, doc_texts, progress, 4, ai_provider)
        
        # æ­¥éª¤6ï¼šç”Ÿæˆä¼šè®®çºªè¦
        meeting_summary = generate_meeting_summary(corrected_transcript, doc_texts, progress, 5, ai_provider)
        
        # æ­¥éª¤7ï¼šç”Ÿæˆæ–‡ä»¶
        progress.start_step(6)
        
        # ç”ŸæˆåŸå§‹è½¬å½•æ–‡ä»¶
        raw_transcript_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_raw_transcript.md')
        with open(raw_transcript_path, 'w', encoding='utf-8') as f:
            f.write(f'# åŸå§‹è¯­éŸ³è½¬å½•\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
            f.write(f'**å¤„ç†è®¾å¤‡**: {whisper_device.upper()}\n')
            f.write(f'**æ£€æµ‹è¯­è¨€**: {detected_lang}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n\n')
            f.write(f'## è½¬å½•å†…å®¹\n\n{transcript}\n')
        
        # ç”Ÿæˆçº æ­£åçš„è½¬å½•æ–‡ä»¶
        corrected_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_corrected_transcript.md')
        with open(corrected_path, 'w', encoding='utf-8') as f:
            f.write(f'# AIçº æ­£åçš„ä¼šè®®è½¬å½•\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
            f.write(f'**å¤„ç†è®¾å¤‡**: {whisper_device.upper()}\n')
            f.write(f'**æ£€æµ‹è¯­è¨€**: {detected_lang}\n')
            f.write(f'**AIæœåŠ¡**: {ai_provider.upper()}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n\n')
            f.write(f'## çº æ­£åçš„è½¬å½•å†…å®¹\n\n{corrected_transcript}\n')
        
        # ç”Ÿæˆå®Œæ•´çš„ä¼šè®®æŠ¥å‘Š
        report_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_meeting_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f'# ä¼šè®®çºªè¦æŠ¥å‘Š\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
            f.write(f'**å¤„ç†è®¾å¤‡**: {whisper_device.upper()}\n')
            f.write(f'**æ£€æµ‹è¯­è¨€**: {detected_lang}\n')
            f.write(f'**AIæœåŠ¡**: {ai_provider.upper()}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n')
            f.write(f'**å‚è€ƒèµ„æ–™**: {len(doc_texts)} ä¸ªæ–‡æ¡£/èƒŒæ™¯ä¿¡æ¯\n\n')
            
            # å¦‚æœæœ‰å‚è€ƒæ–‡æ¡£ï¼Œæ˜¾ç¤ºæ¦‚è§ˆ
            if doc_texts:
                f.write(f'## å‚è€ƒèµ„æ–™æ¦‚è§ˆ\n\n')
                for i, doc in enumerate(doc_texts, 1):
                    # æå–æ–‡æ¡£æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
                    first_line = doc.split('\n')[0] if doc else 'æœªçŸ¥æ–‡æ¡£'
                    f.write(f'{i}. {first_line.replace("##", "").strip()}\n')
                f.write(f'\n---\n\n')
            
            f.write(f'{meeting_summary["summary"]}\n\n')
            f.write(f'---\n\n## é™„å½•ï¼šå®Œæ•´è½¬å½•å†…å®¹\n\n{corrected_transcript}\n')
        
        progress.complete_step(6)
        
        # é€šçŸ¥å¤„ç†å®Œæˆ
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ [{session_id[:8]}] å¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        socketio.emit('processing_complete', {
            'raw_transcript_path': raw_transcript_path,
            'corrected_transcript_path': corrected_path,
            'report_path': report_path,
            'processing_time': total_time,
            'audio_duration': audio_duration,
            'detected_language': detected_lang,
            'ai_provider': ai_provider
        }, to=session_id)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [{session_id[:8]}] å¤„ç†é”™è¯¯: {str(e)}")
        socketio.emit('error', {'message': f'å¤„ç†é”™è¯¯: {str(e)}'}, to=session_id)

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶"""
    try:
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if os.path.exists(file_path):
            return send_file(file_path, as_attachment=True)
        else:
            return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½é”™è¯¯: {e}")
        return jsonify({"error": f"æ–‡ä»¶ä¸‹è½½é”™è¯¯: {str(e)}"}), 500

if __name__ == '__main__':
    # å¯åŠ¨æ—¶æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logger.info("=" * 50)
    logger.info("ğŸ¤ æ™ºèƒ½ä¼šè®®çºªè¦ç”Ÿæˆå™¨å¯åŠ¨")
    logger.info(f"ğŸ–¥ï¸  Whisperè®¾å¤‡: {whisper_device.upper()}")
    logger.info(f"ğŸ¤– OpenAIé…ç½®: {'âœ… å·²é…ç½®' if OPENAI_API_KEY else 'âŒ æœªé…ç½®'}")
    
    # æ£€æŸ¥Ollama
    ollama_models = get_available_ollama_models()
    logger.info(f"ğŸ¦™ Ollamaæ¨¡å‹: {len(ollama_models)} ä¸ªå¯ç”¨")
    for model in ollama_models:
        logger.info(f"   - {model['name']}")
    
    logger.info("=" * 50)
    
    socketio.run(app, debug=True, port=5000)