from flask import Flask, render_template, request, send_file, redirect, url_for, jsonify
from flask_socketio import SocketIO, emit
import os
import subprocess
import whisper
import torch
try:
    import openai
except ImportError:
    openai = None
import requests
import json
from datetime import datetime
import fitz
from fitz import Document
import pytesseract
from PIL import Image
import pandas as pd
import tempfile
from typing import cast, Optional, Dict, List, Tuple, Any
import logging
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
import uuid
import threading
import time
import re
from functools import wraps
import psutil
import base64
from io import BytesIO
from openai.types.chat import ChatCompletionMessageParam
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
app.config['UPLOAD_FOLDER'] = tempfile.mkdtemp()
socketio = SocketIO(app, cors_allowed_origins="*")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'mistral')

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
OPENAI_MODELS = [
    {'id': 'gpt-4o-mini', 'name': 'GPT-4o Mini (æ¨è)', 'provider': 'openai'},
    {'id': 'gpt-4o', 'name': 'GPT-4o', 'provider': 'openai'},
    {'id': 'gpt-4-turbo', 'name': 'GPT-4 Turbo', 'provider': 'openai'},
    {'id': 'gpt-3.5-turbo', 'name': 'GPT-3.5 Turbo', 'provider': 'openai'},
    # OpenRouter models
    {'id': 'deepseek/deepseek-chat', 'name': 'DeepSeek Chat', 'provider': 'openrouter'},
    {'id': 'anthropic/claude-3-haiku', 'name': 'Claude 3 Haiku', 'provider': 'openrouter'},
    {'id': 'meta-llama/llama-3.1-70b-instruct', 'name': 'Llama 3.1 70B', 'provider': 'openrouter'},
    {'id': 'mistralai/mistral-7b-instruct', 'name': 'Mistral 7B Instruct', 'provider': 'openrouter'},
    {'id': 'google/gemini-pro', 'name': 'Gemini Pro', 'provider': 'openrouter'},
]

OLLAMA_DEFAULT_MODELS = [
    {'id': 'mistral', 'name': 'Mistral (æ¨è)', 'size': '7B'},
    {'id': 'deepseek-coder', 'name': 'DeepSeek Coder', 'size': '7B'},
    {'id': 'llama3.1', 'name': 'Llama 3.1', 'size': '8B'},
    {'id': 'qwen2', 'name': 'Qwen2', 'size': '7B'},
    {'id': 'gemma2', 'name': 'Gemma 2', 'size': '9B'},
]

# æ£€æŸ¥GPUå¯ç”¨æ€§
def check_gpu_availability():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒMPS (Apple Silicon)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return 'mps'
        # æ£€æŸ¥CUDA
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'
    except Exception as e:
        logger.warning(f"GPUæ£€æŸ¥å¤±è´¥: {e}")
        return 'cpu'

# åˆå§‹åŒ–Whisperæ¨¡å‹
def initialize_whisper_model():
    """åˆå§‹åŒ–Whisperæ¨¡å‹"""
    try:
        device = check_gpu_availability()
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–Whisperæ¨¡å‹... ä½¿ç”¨è®¾å¤‡: {device}")
        
        if device == 'mps':
            # Apple Silicon MPS
            model = whisper.load_model('medium', device='mps')
        elif device == 'cuda':
            # NVIDIA GPU
            model = whisper.load_model('medium', device='cuda')
        else:
            # CPU
            model = whisper.load_model('medium', device='cpu')
            
        logger.info(f"Whisperæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {device}")
        return model, device
    except Exception as e:
        logger.error(f"Whisperæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        # é™çº§åˆ°CPU
        model = whisper.load_model('medium', device='cpu')
        return model, 'cpu'

whisper_model, whisper_device = initialize_whisper_model()

# é”™è¯¯å¤„ç†è£…é¥°å™¨
def handle_errors(max_retries=3):
    """é”™è¯¯å¤„ç†è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logger.error(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt == max_retries - 1:
                        raise e
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            return None
        return wrapper
    return decorator

# è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
@handle_errors(max_retries=2)
def get_available_ollama_models():
    """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹"""
    try:
        logger.info(f"æ­£åœ¨æ£€æµ‹Ollamaæ¨¡å‹ï¼ŒURL: {OLLAMA_BASE_URL}")
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            available_models = []
            
            if 'models' in data and data['models']:
                # è·å–å·²å®‰è£…çš„æ¨¡å‹åç§°
                installed_names = set()
                for model in data['models']:
                    model_name = model['name'].split(':')[0]
                    installed_names.add(model_name)
                    logger.info(f"æ£€æµ‹åˆ°å·²å®‰è£…çš„Ollamaæ¨¡å‹: {model_name}")
                
                # åŒ¹é…é»˜è®¤æ¨¡å‹åˆ—è¡¨
                for default_model in OLLAMA_DEFAULT_MODELS:
                    if default_model['id'] in installed_names:
                        available_models.append(default_model)
                        logger.info(f"æ·»åŠ å¯ç”¨æ¨¡å‹: {default_model['name']}")
                
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°é»˜è®¤æ¨¡å‹ï¼Œæ·»åŠ æ‰€æœ‰å·²å®‰è£…çš„æ¨¡å‹
                if not available_models:
                    for model in data['models']:
                        model_name = model['name'].split(':')[0]
                        available_models.append({
                            'id': model_name,
                            'name': model_name.title(),
                            'size': 'Unknown'
                        })
                        logger.info(f"æ·»åŠ æ£€æµ‹åˆ°çš„æ¨¡å‹: {model_name}")
            
            logger.info(f"å…±æ£€æµ‹åˆ° {len(available_models)} ä¸ªå¯ç”¨Ollamaæ¨¡å‹")
            return available_models
        else:
            logger.warning(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: HTTP {response.status_code}")
            return []
    except requests.exceptions.ConnectionError:
        logger.warning("æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ")
        return []
    except Exception as e:
        logger.error(f"è·å–Ollamaæ¨¡å‹å¤±è´¥: {e}")
        return []

class ProcessingProgress:
    def __init__(self):
        self.steps = []
        self.current_step = 0
        self.total_steps = 0
        self.session_id: Optional[str] = None
        self.start_time: Optional[float] = None
        self.estimated_total_time: Optional[float] = None
        
    def add_step(self, step_name: str, description: str = "", estimated_duration: float = 0):
        self.steps.append({
            'name': step_name,
            'description': description,
            'status': 'pending',
            'start_time': None,
            'end_time': None,
            'estimated_duration': estimated_duration,
            'progress_percentage': 0
        })
        self.total_steps = len(self.steps)
        
    def start_step(self, step_index: int):
        if 0 <= step_index < len(self.steps):
            self.current_step = step_index
            self.steps[step_index]['status'] = 'processing'
            self.steps[step_index]['start_time'] = time.time()
            
            if self.start_time is None:
                self.start_time = time.time()
                
            step_name = self.steps[step_index]['name']
            logger.info(f"ğŸš€ [{self.session_id[:8] if self.session_id else 'LOCAL'}] å¼€å§‹: {step_name}")
            self.emit_progress()
            
    def update_step_progress(self, step_index: int, progress: float):
        """æ›´æ–°æ­¥éª¤å†…éƒ¨è¿›åº¦"""
        if 0 <= step_index < len(self.steps):
            self.steps[step_index]['progress_percentage'] = min(100, max(0, progress))
            self.emit_progress()
            
    def complete_step(self, step_index: int):
        if 0 <= step_index < len(self.steps):
            self.steps[step_index]['status'] = 'completed'
            self.steps[step_index]['end_time'] = time.time()
            self.steps[step_index]['progress_percentage'] = 100
            
            step_name = self.steps[step_index]['name']
            duration = self.steps[step_index]['end_time'] - self.steps[step_index]['start_time']
            logger.info(f"âœ… [{self.session_id[:8] if self.session_id else 'LOCAL'}] å®Œæˆ: {step_name} (è€—æ—¶ {duration:.1f}ç§’)")
            self.emit_progress()
            
    def estimate_remaining_time(self):
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        if not self.start_time or self.total_steps == 0:
            return None
        
        # ä½¿ç”¨é¢„è®¾çš„ä¼°ç®—æ—¶é•¿æ¥è®¡ç®—
        total_estimated_duration = sum(step.get('estimated_duration', 0) for step in self.steps)
        if total_estimated_duration == 0:
            return None # æ— æ³•é¢„ä¼°
            
        completed_duration = 0
        for step in self.steps:
            if step['status'] == 'completed':
                completed_duration += step.get('estimated_duration', 0)
        
        remaining_duration = total_estimated_duration - completed_duration
        
        # å¯¹äºæ­£åœ¨å¤„ç†çš„æ­¥éª¤ï¼Œå¯ä»¥åŠ å…¥æ›´ç²¾ç»†çš„ä¼°ç®—
        if 0 <= self.current_step < len(self.steps) and self.steps[self.current_step]['status'] == 'processing':
            current_step_progress = self.steps[self.current_step].get('progress_percentage', 0) / 100
            current_step_estimated_duration = self.steps[self.current_step].get('estimated_duration', 0)
            remaining_duration -= current_step_progress * current_step_estimated_duration

        return max(0, remaining_duration)
            
    def emit_progress(self):
        if self.session_id:
            # è®¡ç®—æ€»ä½“è¿›åº¦
            total_estimated_duration = sum(step.get('estimated_duration', 0) for step in self.steps)
            completed_duration = 0
            
            for step in self.steps:
                if step['status'] == 'completed':
                    completed_duration += step.get('estimated_duration', 0)
                elif step['status'] == 'processing':
                    progress_percentage = step.get('progress_percentage', 0)
                    estimated_duration = step.get('estimated_duration', 0)
                    completed_duration += (progress_percentage / 100) * estimated_duration
            
            overall_percentage = (completed_duration / total_estimated_duration) * 100 if total_estimated_duration > 0 else 0
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´
            remaining_time = self.estimate_remaining_time()
            
            # å¤„ç†datetimeåºåˆ—åŒ–é—®é¢˜
            serialized_steps = []
            for step in self.steps:
                serialized_step = {
                    'name': step['name'],
                    'description': step['description'],
                    'status': step['status'],
                    'progress_percentage': step['progress_percentage'],
                    'start_time': step['start_time'],
                    'end_time': step['end_time']
                }
                serialized_steps.append(serialized_step)
            
            progress_data = {
                'current_step': self.current_step,
                'total_steps': self.total_steps,
                'steps': serialized_steps,
                'percentage': overall_percentage,
                'estimated_remaining_time': remaining_time
            }
            
            # å‘½ä»¤è¡Œç›‘æ§è¾“å‡º
            current_step_name = self.steps[self.current_step]['name'] if self.current_step < len(self.steps) else "å®Œæˆ"
            remaining_str = f", é¢„è®¡å‰©ä½™ {remaining_time:.1f}ç§’" if remaining_time else ""
            logger.info(f"ğŸ”„ [{self.session_id[:8]}] è¿›åº¦: {overall_percentage:.1f}% - {current_step_name}{remaining_str}")
            
            socketio.emit('progress_update', progress_data, to=self.session_id)

@handle_errors(max_retries=3)
def call_openai_api(prompt: str, model: Optional[str] = None, max_tokens: int = 2000, image_base64: Optional[str] = None) -> Optional[str]:
    """è°ƒç”¨OpenAI APIæˆ–OpenRouter APIï¼Œæ”¯æŒå¤šæ¨¡æ€"""
    try:
        if not openai:
            logger.error("ğŸš« OpenAI library not available - please install: pip install openai")
            return None
            
        if not OPENAI_API_KEY:
            logger.error("ğŸ”‘ OPENAI_API_KEY not configured in .env file")
            return None
            
        used_model = model or OPENAI_MODEL
        logger.info(f"ğŸ¤– è°ƒç”¨OpenAI API - æ¨¡å‹: {used_model}, åŸºç¡€URL: {OPENAI_BASE_URL}")
        logger.debug(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # æ£€æŸ¥æç¤ºè¯é•¿åº¦
        if len(prompt) > 100000:  # çº¦100kå­—ç¬¦ä¸Šé™
            logger.warning(f"âš ï¸ æç¤ºè¯è¿‡é•¿ ({len(prompt)} å­—ç¬¦)ï¼Œå¯èƒ½å½±å“å¤„ç†æ•ˆæœ")
            # æˆªæ–­æç¤ºè¯
            prompt = prompt[:100000] + "\n\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"
        
        # éªŒè¯æç¤ºè¯å†…å®¹
        if not prompt.strip():
            logger.error("âŒ æç¤ºè¯ä¸ºç©º")
            return None
        
        client = openai.OpenAI(
            api_key=OPENAI_API_KEY,
            base_url=OPENAI_BASE_URL
        )

        messages: List[ChatCompletionMessageParam] = [
            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ä¼šè®®è®°å½•å¤„ç†ä¸“å®¶ï¼Œæ“…é•¿è½¬å½•çº é”™å’Œä¼šè®®çºªè¦ç”Ÿæˆã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚"},
        ]

        if image_base64:
            # å¤šæ¨¡æ€è°ƒç”¨
            used_model = 'gpt-4o' # å¼ºåˆ¶ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹
            messages.append({
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}"
                        }
                    }
                ]
            })
        else:
            # çº¯æ–‡æœ¬è°ƒç”¨
            messages.append({"role": "user", "content": prompt})

        response = client.chat.completions.create(
            model=used_model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=0.3,  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
            timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
        )
        
        content = response.choices[0].message.content
        result = content.strip() if content else ""
        
        # éªŒè¯è¿”å›ç»“æœ
        if not result:
            logger.error("âŒ OpenAI API è¿”å›ç©ºç»“æœ")
            return None
            
        if len(result) < 20:  # ç»“æœå¤ªçŸ­å¯èƒ½æœ‰é—®é¢˜
            logger.warning(f"âš ï¸ OpenAI API è¿”å›ç»“æœè¾ƒçŸ­: {len(result)} å­—ç¬¦")
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
        if "Error" in result or "error" in result or "é”™è¯¯" in result:
            logger.warning(f"âš ï¸ OpenAIè¿”å›å†…å®¹å¯èƒ½åŒ…å«é”™è¯¯ä¿¡æ¯: {result[:200]}...")
            
        logger.info(f"âœ… OpenAI API è°ƒç”¨æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
        return result
        
    except Exception as e:
        logger.error(f"âŒ OpenAI API è°ƒç”¨å¤±è´¥: {str(e)}")
        # è®°å½•æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        response = getattr(e, 'response', None)
        if response:
            logger.error(f"HTTPçŠ¶æ€ç : {getattr(response, 'status_code', 'unknown')}")
        return None

@handle_errors(max_retries=3)
def call_ollama_api(prompt: str, model: Optional[str] = None, max_tokens: int = 2000) -> Optional[str]:
    """è°ƒç”¨æœ¬åœ°ollamaæ¨¡å‹"""
    try:
        used_model = model or OLLAMA_MODEL
        logger.info(f"ğŸ¦™ è°ƒç”¨Ollama API - æ¨¡å‹: {used_model}, è¾“å…¥é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # éªŒè¯æç¤ºè¯å†…å®¹
        if not prompt.strip():
            logger.error("âŒ æç¤ºè¯ä¸ºç©º")
            return None
        
        # æ£€æŸ¥æç¤ºè¯é•¿åº¦
        if len(prompt) > 50000:  # Ollamaé€šå¸¸æ”¯æŒçš„ä¸Šä¸‹æ–‡æ›´å°
            logger.warning(f"âš ï¸ æç¤ºè¯è¿‡é•¿ ({len(prompt)} å­—ç¬¦)ï¼Œæˆªæ–­å¤„ç†")
            prompt = prompt[:50000] + "\n\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"
        
        # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
        try:
            health_response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
            if health_response.status_code != 200:
                logger.error(f"âŒ OllamaæœåŠ¡ä¸å¯ç”¨ï¼ŒçŠ¶æ€ç : {health_response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡: {e}")
            return None
        
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": used_model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": max_tokens,
                    "temperature": 0.3,  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
                    "top_p": 0.9,
                    "stop": ["<|im_end|>", "<|endoftext|>", "###", "---"]  # æ·»åŠ åœæ­¢è¯
                }
            },
            timeout=300
        )
        
        if response.status_code == 200:
            result = response.json().get('response', '').strip()
            
            # éªŒè¯è¿”å›ç»“æœ
            if not result:
                logger.error("âŒ Ollama API è¿”å›ç©ºç»“æœ")
                return None
                
            if len(result) < 20:  # ç»“æœå¤ªçŸ­å¯èƒ½æœ‰é—®é¢˜
                logger.warning(f"âš ï¸ Ollama API è¿”å›ç»“æœè¾ƒçŸ­: {len(result)} å­—ç¬¦")
                
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
            if "Error" in result or "error" in result or "é”™è¯¯" in result:
                logger.warning(f"âš ï¸ Ollamaè¿”å›å†…å®¹å¯èƒ½åŒ…å«é”™è¯¯ä¿¡æ¯: {result[:200]}...")
                
            logger.info(f"âœ… Ollama API è°ƒç”¨æˆåŠŸ - è¾“å‡ºé•¿åº¦: {len(result)} å­—ç¬¦")
            return result
        else:
            error_msg = response.json().get('error', 'æœªçŸ¥é”™è¯¯') if response.content else 'æ— å“åº”å†…å®¹'
            logger.error(f"âŒ Ollama API è°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, é”™è¯¯: {error_msg}")
            return None
            
    except Exception as e:
        logger.error(f"âŒ Ollama API è°ƒç”¨å¼‚å¸¸: {str(e)}")
        return None

def chunk_transcript(transcript: str, max_chunk_size: int = 8000) -> List[str]:
    """å°†é•¿è½¬å½•æ–‡æœ¬åˆ†å‰²æˆé€‚åˆAIå¤„ç†çš„å—"""
    if len(transcript) <= max_chunk_size:
        return [transcript]
    
    logger.info(f"ğŸ“„ å¼€å§‹åˆ†å‰²é•¿æ–‡æœ¬: {len(transcript)} å­—ç¬¦ -> ç›®æ ‡å¤§å°: {max_chunk_size}")
    
    # å°è¯•æŒ‰æ®µè½åˆ†å‰²
    paragraphs = transcript.split('\n\n')
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        # æ£€æŸ¥å•ä¸ªæ®µè½æ˜¯å¦è¿‡é•¿
        if len(paragraph) > max_chunk_size:
            # å¦‚æœå½“å‰chunkä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜
            if current_chunk:
                chunks.append(current_chunk)
                current_chunk = ""
            
            # æŒ‰å¥å­åˆ†å‰²è¿‡é•¿çš„æ®µè½
            sentences = paragraph.split('ã€‚')
            for sentence in sentences:
                if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
                    if current_chunk:
                        current_chunk += 'ã€‚' + sentence
                    else:
                        current_chunk = sentence
                else:
                    if current_chunk:
                        chunks.append(current_chunk + 'ã€‚')
                    current_chunk = sentence
        else:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥åŠ å…¥å½“å‰chunk
            if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:
                if current_chunk:
                    current_chunk += '\n\n' + paragraph
                else:
                    current_chunk = paragraph
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph
    
    # ä¿å­˜æœ€åä¸€ä¸ªchunk
    if current_chunk:
        chunks.append(current_chunk)
    
    # å¦‚æœä»æœ‰è¿‡é•¿çš„å—ï¼Œè¿›è¡Œå¼ºåˆ¶åˆ†å‰²
    final_chunks = []
    for chunk in chunks:
        if len(chunk) <= max_chunk_size:
            final_chunks.append(chunk)
        else:
            # å¼ºåˆ¶æŒ‰å­—ç¬¦åˆ†å‰²
            logger.warning(f"âš ï¸ å¼ºåˆ¶åˆ†å‰²è¿‡é•¿å—: {len(chunk)} å­—ç¬¦")
            for i in range(0, len(chunk), max_chunk_size):
                sub_chunk = chunk[i:i + max_chunk_size]
                final_chunks.append(sub_chunk)
    
    logger.info(f"âœ… é•¿æ–‡æœ¬åˆ†å‰²å®Œæˆ: åŸé•¿åº¦ {len(transcript)}, åˆ†æˆ {len(final_chunks)} å—")
    for i, chunk in enumerate(final_chunks):
        logger.debug(f"   å— {i+1}: {len(chunk)} å­—ç¬¦")
    
    return final_chunks

def process_long_transcript_correction(transcript: str, context: str, reference_docs: List[str], 
                                     progress: ProcessingProgress, step_index: int, 
                                     ai_provider: str, model: str) -> str:
    """å¤„ç†é•¿è½¬å½•æ–‡æœ¬çš„çº é”™"""
    # å¦‚æœæ–‡æœ¬ä¸é•¿ï¼Œç›´æ¥å¤„ç†
    if len(transcript) <= 12000:
        # ç›´æ¥è°ƒç”¨åº•å±‚å‡½æ•°ï¼Œé¿å…é€’å½’
        return _single_transcript_correction(transcript, context, reference_docs, 
                                           progress, step_index, ai_provider, model)
    
    logger.info(f"ğŸ“„ æ£€æµ‹åˆ°é•¿æ–‡æœ¬ ({len(transcript)} å­—ç¬¦)ï¼Œä½¿ç”¨åˆ†å—å¤„ç†")
    progress.start_step(step_index)
    
    # åˆ†å—å¤„ç†
    chunks = chunk_transcript(transcript, max_chunk_size=10000)
    corrected_chunks = []
    
    # å‡†å¤‡ç®€åŒ–çš„å‚è€ƒæ–‡æ¡£ï¼ˆé¿å…æ¯æ¬¡éƒ½å‘é€å®Œæ•´æ–‡æ¡£ï¼‰
    simplified_reference = reference_docs[:3] if reference_docs else []  # åªå–å‰3ä¸ªæ–‡æ¡£
    
    for i, chunk in enumerate(chunks):
        logger.info(f"ğŸ“ å¤„ç†ç¬¬ {i+1}/{len(chunks)} å— ({len(chunk)} å­—ç¬¦)")
        progress.update_step_progress(step_index, 20 + (i * 60 // len(chunks)))
        
        # ä¸ºæ¯ä¸ªå—åˆ›å»ºç®€åŒ–çš„ä¸Šä¸‹æ–‡
        chunk_context = f"è¿™æ˜¯ä¼šè®®è½¬å½•çš„ç¬¬{i+1}éƒ¨åˆ†ï¼Œå…±{len(chunks)}éƒ¨åˆ†ã€‚{context}"
        
        # å¤„ç†å•ä¸ªå— - ä½¿ç”¨å†…éƒ¨å‡½æ•°é¿å…é€’å½’
        try:
            corrected_chunk = _single_transcript_correction(
                chunk, chunk_context, simplified_reference, 
                ProcessingProgress(), 0,  # ä½¿ç”¨ä¸´æ—¶è¿›åº¦å¯¹è±¡
                ai_provider, model
            )
            corrected_chunks.append(corrected_chunk)
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ç¬¬ {i+1} å—æ—¶å‡ºé”™: {str(e)}")
            corrected_chunks.append(chunk)  # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹å—
    
    # åˆå¹¶å¤„ç†ç»“æœ
    try:
        # æ™ºèƒ½åˆå¹¶ï¼Œä¿æŒæ®µè½ç»“æ„
        final_result = ""
        for i, chunk in enumerate(corrected_chunks):
            if i > 0:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ åˆ†éš”ç¬¦
                if not chunk.startswith('\n') and not final_result.endswith('\n'):
                    final_result += '\n\n'
            final_result += chunk
        
        progress.update_step_progress(step_index, 90)
        
        # åŸºæœ¬çš„åå¤„ç†
        final_result = final_result.strip()
        
        # å»é™¤å¯èƒ½çš„é‡å¤å†…å®¹
        lines = final_result.split('\n')
        seen_lines = set()
        unique_lines = []
        for line in lines:
            if line.strip() and line not in seen_lines:
                seen_lines.add(line)
                unique_lines.append(line)
            elif not line.strip():  # ä¿ç•™ç©ºè¡Œ
                unique_lines.append(line)
        
        final_result = '\n'.join(unique_lines)
        
    except Exception as e:
        logger.error(f"âŒ åˆå¹¶å¤„ç†ç»“æœæ—¶å‡ºé”™: {str(e)}")
        final_result = '\n\n'.join(corrected_chunks)
    
    progress.complete_step(step_index)
    
    logger.info(f"âœ… é•¿æ–‡æœ¬åˆ†å—å¤„ç†å®Œæˆ: {len(chunks)} å— -> {len(final_result)} å­—ç¬¦")
    return final_result

def _single_transcript_correction(transcript: str, context: str, reference_docs: List[str], 
                                progress: ProcessingProgress, step_index: int, 
                                ai_provider: str, model: str) -> str:
    """å•æ¬¡è½¬å½•çº é”™ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œé¿å…é€’å½’ï¼‰"""
    progress.start_step(step_index)
    
    try:
        # éªŒè¯è¾“å…¥
        if not transcript or len(transcript.strip()) < 10:
            logger.warning("âš ï¸ è½¬å½•æ–‡æœ¬è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡AIçº é”™")
            progress.complete_step(step_index)
            return transcript
        
        # éªŒè¯æç¤ºè¯æ¨¡æ¿
        if not correction_prompt_template:
            logger.error("âŒ è½¬å½•çº é”™æç¤ºè¯æœªåŠ è½½ï¼Œè¿”å›åŸå§‹è½¬å½•")
            progress.complete_step(step_index)
            return transcript
        
        # å‡†å¤‡å‚è€ƒå†…å®¹
        reference_content = "\n\n---\n\n".join(reference_docs) if reference_docs else "æ— å‚è€ƒæ–‡æ¡£"
        context_content = context if context and context.strip() else "æ— ç‰¹å®šèƒŒæ™¯ä¿¡æ¯"
        
        # æ„å»ºæç¤ºè¯
        try:
            prompt = correction_prompt_template.replace("{{transcript}}", transcript) \
                                               .replace("{{context}}", context_content) \
                                               .replace("{{reference_docs}}", reference_content)
        except Exception as e:
            logger.error(f"âŒ æ„å»ºæç¤ºè¯å¤±è´¥: {str(e)}")
            progress.complete_step(step_index)
            return transcript
        
        # éªŒè¯æç¤ºè¯å®Œæ•´æ€§
        if "{{" in prompt or "}}" in prompt:
            logger.warning("âš ï¸ æç¤ºè¯å˜é‡æ›¿æ¢ä¸å®Œæ•´ï¼Œå¯èƒ½å½±å“æ•ˆæœ")
        
        logger.info(f"ğŸ“ å¼€å§‹AIçº é”™ - è½¬å½•é•¿åº¦: {len(transcript)}, æç¤ºè¯é•¿åº¦: {len(prompt)}")
        progress.update_step_progress(step_index, 30)
        
        corrected_text = None
        
        # å°è¯•è°ƒç”¨AIæœåŠ¡
        if ai_provider == 'openai' and OPENAI_API_KEY:
            logger.info("ğŸ¤– ä½¿ç”¨OpenAIè¿›è¡Œè½¬å½•çº é”™")
            corrected_text = call_openai_api(prompt, model=model, max_tokens=6000)
        elif ai_provider == 'ollama':
            logger.info("ğŸ¦™ ä½¿ç”¨Ollamaè¿›è¡Œè½¬å½•çº é”™")
            corrected_text = call_ollama_api(prompt, model=model, max_tokens=6000)
        else:
            logger.error(f"âŒ æœªé…ç½®æœ‰æ•ˆçš„AIæœåŠ¡: {ai_provider}")
        
        progress.update_step_progress(step_index, 80)
        
        # éªŒè¯AIè¿”å›ç»“æœ
        if not corrected_text:
            logger.warning("âš ï¸ AIçº é”™å¤±è´¥ï¼Œè¿”å›åŸå§‹è½¬å½•")
            corrected_text = transcript
        elif len(corrected_text.strip()) < len(transcript) * 0.2:  # çº é”™åæ–‡æœ¬è¿‡çŸ­
            logger.warning("âš ï¸ AIçº é”™ç»“æœå¼‚å¸¸ï¼ˆè¿‡çŸ­ï¼‰ï¼Œè¿”å›åŸå§‹è½¬å½•")
            corrected_text = transcript
        else:
            # åŸºæœ¬çš„è´¨é‡æ£€æŸ¥
            improvement_ratio = len(corrected_text) / len(transcript)
            if improvement_ratio > 5:  # çº é”™åæ–‡æœ¬è¿‡é•¿å¯èƒ½æœ‰é—®é¢˜
                logger.warning(f"âš ï¸ AIçº é”™ç»“æœå¼‚å¸¸ï¼ˆè¿‡é•¿ {improvement_ratio:.1f}xï¼‰ï¼Œè¿”å›åŸå§‹è½¬å½•")
                corrected_text = transcript
            else:
                # æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ‰æ„ä¹‰çš„æ–‡æœ¬
                if corrected_text.count('\n') > len(transcript) * 0.1:  # æ¢è¡Œè¿‡å¤š
                    logger.warning("âš ï¸ AIçº é”™ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè¿”å›åŸå§‹è½¬å½•")
                    corrected_text = transcript
                else:
                    logger.info(f"âœ… AIçº é”™å®Œæˆ - åŸæ–‡: {len(transcript)} å­—ç¬¦, çº é”™å: {len(corrected_text)} å­—ç¬¦")
        
        progress.complete_step(step_index)
        return corrected_text
        
    except Exception as e:
        logger.error(f"âŒ è½¬å½•çº é”™è¿‡ç¨‹å‡ºé”™: {str(e)}")
        progress.complete_step(step_index)
        return transcript  # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›åŸå§‹è½¬å½•

@handle_errors(max_retries=3)
def advanced_transcription_correction(transcript: str, context: str, reference_docs: List[str], progress: ProcessingProgress, step_index: int, ai_provider: str, model: str) -> str:
    """ä½¿ç”¨AIçº æ­£è½¬å½•æ–‡æœ¬ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡å’Œå‚è€ƒæ–‡æ¡£"""
    # å¯¹äºé•¿æ–‡æœ¬ï¼Œä½¿ç”¨åˆ†å—å¤„ç†
    if len(transcript) > 15000:
        return process_long_transcript_correction(transcript, context, reference_docs, 
                                                progress, step_index, ai_provider, model)
    else:
        return _single_transcript_correction(transcript, context, reference_docs, 
                                           progress, step_index, ai_provider, model)

@handle_errors(max_retries=3)
def generate_meeting_summary(corrected_transcript: str, context: str, reference_docs: List[str], progress: ProcessingProgress, step_index: int, ai_provider: str, model: str) -> Dict[str, str]:
    """ä½¿ç”¨AIç”Ÿæˆä¼šè®®çºªè¦"""
    progress.start_step(step_index)
    
    try:
        # éªŒè¯è¾“å…¥
        if not corrected_transcript or len(corrected_transcript.strip()) < 50:
            logger.warning("âš ï¸ çº é”™åè½¬å½•æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæœ‰æ•ˆçºªè¦")
            progress.complete_step(step_index)
            return {"summary": create_fallback_summary(corrected_transcript, context)}
        
        # éªŒè¯æç¤ºè¯æ¨¡æ¿
        if not summary_prompt_template:
            logger.error("âŒ ä¼šè®®çºªè¦æç¤ºè¯æœªåŠ è½½ï¼Œåˆ›å»ºåŸºç¡€çºªè¦")
            progress.complete_step(step_index)
            return {"summary": create_fallback_summary(corrected_transcript, context)}
        
        # å‡†å¤‡å‚è€ƒå†…å®¹
        reference_content = "\n\n---\n\n".join(reference_docs) if reference_docs else "æ— å‚è€ƒæ–‡æ¡£"
        context_content = context if context and context.strip() else "æ— ç‰¹å®šèƒŒæ™¯ä¿¡æ¯"
        
        # æ„å»ºæç¤ºè¯
        try:
            prompt = summary_prompt_template.replace("{{corrected_transcript}}", corrected_transcript) \
                                            .replace("{{context}}", context_content) \
                                            .replace("{{reference_docs}}", reference_content)
        except Exception as e:
            logger.error(f"âŒ æ„å»ºçºªè¦æç¤ºè¯å¤±è´¥: {str(e)}")
            progress.complete_step(step_index)
            return {"summary": create_fallback_summary(corrected_transcript, context)}
        
        # éªŒè¯æç¤ºè¯å®Œæ•´æ€§
        if "{{" in prompt or "}}" in prompt:
            logger.warning("âš ï¸ çºªè¦æç¤ºè¯å˜é‡æ›¿æ¢ä¸å®Œæ•´ï¼Œå¯èƒ½å½±å“æ•ˆæœ")
        
        logger.info(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆä¼šè®®çºªè¦ - è½¬å½•é•¿åº¦: {len(corrected_transcript)}, æç¤ºè¯é•¿åº¦: {len(prompt)}")
        progress.update_step_progress(step_index, 30)
        
        summary = None
        
        # å°è¯•è°ƒç”¨AIæœåŠ¡
        if ai_provider == 'openai' and OPENAI_API_KEY:
            logger.info("ğŸ¤– ä½¿ç”¨OpenAIç”Ÿæˆä¼šè®®çºªè¦")
            summary = call_openai_api(prompt, model=model, max_tokens=5000)
        elif ai_provider == 'ollama':
            logger.info("ğŸ¦™ ä½¿ç”¨Ollamaç”Ÿæˆä¼šè®®çºªè¦")
            summary = call_ollama_api(prompt, model=model, max_tokens=5000)
        else:
            logger.error(f"âŒ æœªé…ç½®æœ‰æ•ˆçš„AIæœåŠ¡: {ai_provider}")
        
        progress.update_step_progress(step_index, 80)
        
        # éªŒè¯AIè¿”å›ç»“æœ
        if not summary:
            logger.warning("âš ï¸ AIçºªè¦ç”Ÿæˆå¤±è´¥ï¼Œåˆ›å»ºåŸºç¡€çºªè¦")
            summary = create_fallback_summary(corrected_transcript, context)
        elif len(summary.strip()) < 100:  # çºªè¦è¿‡çŸ­
            logger.warning("âš ï¸ AIç”Ÿæˆçš„çºªè¦è¿‡çŸ­ï¼Œåˆ›å»ºåŸºç¡€çºªè¦")
            summary = create_fallback_summary(corrected_transcript, context)
        else:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„çºªè¦ç»“æ„
            required_keywords = ["ä¼šè®®", "è®®é¢˜", "è®¨è®º", "å†³ç­–", "è¡ŒåŠ¨", "æ¦‚è§ˆ", "è¦ç‚¹"]
            if not any(keyword in summary for keyword in required_keywords):
                logger.warning("âš ï¸ AIç”Ÿæˆçš„çºªè¦ç¼ºå°‘å…³é”®ç»“æ„ï¼Œåˆ›å»ºåŸºç¡€çºªè¦")
                summary = create_fallback_summary(corrected_transcript, context)
            else:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«markdownæ ¼å¼
                if "##" not in summary and "**" not in summary:
                    logger.warning("âš ï¸ AIç”Ÿæˆçš„çºªè¦ç¼ºå°‘æ ¼å¼åŒ–ï¼Œä½†å†…å®¹å¯ç”¨")
                
                # æ£€æŸ¥é•¿åº¦æ˜¯å¦åˆç†
                if len(summary) > len(corrected_transcript) * 2:
                    logger.warning("âš ï¸ AIç”Ÿæˆçš„çºªè¦è¿‡é•¿ï¼Œå¯èƒ½æœ‰é‡å¤å†…å®¹")
                
                logger.info(f"âœ… ä¼šè®®çºªè¦ç”Ÿæˆå®Œæˆ - é•¿åº¦: {len(summary)} å­—ç¬¦")
        
        progress.complete_step(step_index)
        return {"summary": summary}
        
    except Exception as e:
        logger.error(f"âŒ ä¼šè®®çºªè¦ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {str(e)}")
        progress.complete_step(step_index)
        return {"summary": create_fallback_summary(corrected_transcript, context)}

def create_fallback_summary(transcript: str, context: str) -> str:
    """åˆ›å»ºåŸºç¡€ä¼šè®®çºªè¦ï¼ˆå½“AIç”Ÿæˆå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    try:
        # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
        word_count = len(transcript)
        estimated_duration = word_count // 200  # ä¼°ç®—ä¼šè®®æ—¶é•¿ï¼ˆæŒ‰200å­—/åˆ†é’Ÿï¼‰
        
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        common_meeting_terms = ["é¡¹ç›®", "ä»»åŠ¡", "è®¡åˆ’", "ç›®æ ‡", "é—®é¢˜", "æ–¹æ¡ˆ", "å»ºè®®", "å†³å®š", "å®‰æ’", "æ—¶é—´", "è®¨è®º", "ä¼šè®®", "å›¢é˜Ÿ", "è¿›åº¦", "å®Œæˆ", "éœ€è¦", "è€ƒè™‘", "ç¡®è®¤"]
        for term in common_meeting_terms:
            if term in transcript:
                keywords.append(term)
        
        # å°è¯•æå–å¯èƒ½çš„å‚ä¸è€…
        participants = []
        import re
        # ç®€å•çš„å§“åæ¨¡å¼åŒ¹é…
        name_patterns = [
            r'[å¼ æç‹åˆ˜é™ˆæ¨èµµé»„å‘¨å´å¾å­™æœ±é©¬èƒ¡éƒ­æ—ä½•é«˜ç½—éƒ‘æ¢è°¢][A-Za-z\u4e00-\u9fff]{1,3}',
            r'[A-Z][a-z]{2,8}',
        ]
        for pattern in name_patterns:
            matches = re.findall(pattern, transcript)
            participants.extend(matches[:5])  # æœ€å¤š5ä¸ª
        
        # å»é‡
        participants = list(set(participants))
        
        # æå–å¯èƒ½çš„æ—¶é—´ä¿¡æ¯
        time_patterns = [
            r'\d{1,2}[æœˆ]\d{1,2}[æ—¥]',
            r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',
            r'[æœ¬ä¸Šä¸‹][å‘¨æœˆå¹´]',
            r'[æ˜ä»Šæ˜¨][å¤©æ—¥]',
            r'\d{1,2}[ç‚¹æ—¶]',
        ]
        time_mentions = []
        for pattern in time_patterns:
            matches = re.findall(pattern, transcript)
            time_mentions.extend(matches[:3])  # æœ€å¤š3ä¸ª
        
        # æ„å»ºåŸºç¡€çºªè¦
        summary = f"""# ä¼šè®®çºªè¦

## ä¼šè®®æ¦‚è§ˆ
- **æ—¶é—´**: {', '.join(time_mentions) if time_mentions else 'å¾…è¡¥å……'}
- **é¢„è®¡æ—¶é•¿**: çº¦ {estimated_duration} åˆ†é’Ÿ
- **ä¸»è¦å†…å®¹**: {context if context else 'å·¥ä½œä¼šè®®è®¨è®º'}
- **å‚ä¸äººå‘˜**: {', '.join(participants) if participants else 'å¾…è¡¥å……'}

## è®¨è®ºè¦ç‚¹
{transcript[:800]}{'...' if len(transcript) > 800 else ''}

## å…³é”®ä¿¡æ¯
- **æ¶‰åŠå…³é”®è¯**: {', '.join(keywords[:10]) if keywords else 'å¾…åˆ†æ'}
- **å†…å®¹é•¿åº¦**: {word_count} å­—ç¬¦
- **å¤„ç†çŠ¶æ€**: è‡ªåŠ¨ç”ŸæˆåŸºç¡€ç‰ˆæœ¬

## åç»­è·Ÿè¿›
- æœ¬çºªè¦ä¸ºç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆçš„åŸºç¡€ç‰ˆæœ¬
- å»ºè®®äººå·¥è¿›ä¸€æ­¥å®Œå–„å’Œè¡¥å……
- å¦‚éœ€è¯¦ç»†åˆ†æï¼Œè¯·é‡æ–°å°è¯•AIç”ŸæˆåŠŸèƒ½

---
*æœ¬çºªè¦ç”±AIä¼šè®®åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*"""
        
        logger.info(f"âœ… åˆ›å»ºåŸºç¡€çºªè¦å®Œæˆ - é•¿åº¦: {len(summary)} å­—ç¬¦")
        return summary
        
    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºåŸºç¡€çºªè¦å¤±è´¥: {str(e)}")
        return f"""# ä¼šè®®çºªè¦

## ä¼šè®®å†…å®¹
{transcript[:1000]}{'...' if len(transcript) > 1000 else ''}

## å¤„ç†è¯´æ˜
- æœ¬çºªè¦ä¸ºåŸå§‹è½¬å½•å†…å®¹
- ç”±äºç³»ç»Ÿå¤„ç†å¼‚å¸¸ï¼Œæœªèƒ½ç”Ÿæˆç»“æ„åŒ–çºªè¦
- å»ºè®®äººå·¥æ•´ç†æˆ–é‡æ–°å¤„ç†

---
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*"""

def extract_text_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
    try:
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            doc = fitz.open(file_path)
            text = ""
            for page_num in range(len(doc)):
                page = doc[page_num]
                text += page.get_text()  # type: ignore
            doc.close()
            return text
        elif file_ext in ['.md', '.txt']:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            logger.warning(f"Unsupported file type: {file_ext}")
            return ""
    except Exception as e:
        logger.error(f"Error extracting text from {file_path}: {str(e)}")
        return ""

def estimate_audio_duration(audio_path: str) -> float:
    """ä¼°ç®—éŸ³é¢‘æ—¶é•¿"""
    try:
        result = subprocess.run(
            ['ffprobe', '-v', 'quiet', '-show_entries', 'format=duration', '-of', 'csv=p=0', audio_path],
            capture_output=True, text=True
        )
        return float(result.stdout.strip())
    except Exception as e:
        logger.error(f"æ— æ³•è·å–éŸ³é¢‘æ—¶é•¿: {e}")
        return 0

@app.route('/')
def index():
    # è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
    available_ollama_models = get_available_ollama_models()
    
    # æ£€æŸ¥é…ç½®çŠ¶æ€
    config_status = {
        'openai_configured': bool(OPENAI_API_KEY),
        'ollama_available': len(available_ollama_models) > 0,
        'whisper_device': whisper_device
    }
    
    return render_template('index.html', 
                         openai_models=OPENAI_MODELS,
                         ollama_models=available_ollama_models,
                         config_status=config_status,
                         whisper_device=whisper_device)

@app.route('/get_models')
def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    available_ollama_models = get_available_ollama_models()
    config_status = {
        'openai_configured': bool(OPENAI_API_KEY),
        'ollama_available': len(available_ollama_models) > 0
    }
    
    return jsonify({
        'openai_models': OPENAI_MODELS,
        'ollama_models': available_ollama_models,
        'config_status': config_status,
        'whisper_device': whisper_device
    })

@socketio.on('connect')
def handle_connect():
    logger.info(f"Client connected: {request.sid}")  # type: ignore

@socketio.on('disconnect')
def handle_disconnect():
    logger.info(f"Client disconnected: {request.sid}")  # type: ignore

@socketio.on('join')
def handle_join(session_id):
    """å®¢æˆ·ç«¯åŠ å…¥ä¼šè¯æˆ¿é—´"""
    from flask_socketio import join_room
    join_room(session_id)
    logger.info(f"Client {request.sid} joined session: {session_id}")  # type: ignore
    emit('joined', {'session_id': session_id})

@app.route('/process', methods=['POST'])
def process_meeting():
    session_id = str(uuid.uuid4())
    try:
        video = request.files.get('video')
        docs = request.files.getlist('docs[]')
        context_input = request.form.get('context', '')
        ai_provider = request.form.get('aiProvider', 'openai')
        
        # Determine the model to use
        if ai_provider == 'openai':
            model = request.form.get('model', OPENAI_MODELS[0]['id'])
        else:
            # Fallback for ollama if no model is selected or available
            available_ollama = get_available_ollama_models()
            model = request.form.get('model', available_ollama[0]['id'] if available_ollama else OLLAMA_DEFAULT_MODELS[0]['id'])

        logger.info(f"æ”¶åˆ°æ–°çš„å¤„ç†è¯·æ±‚ - Session: {session_id[:8]}, AI: {ai_provider}, Model: {model}")
        if video and video.filename:
            logger.info(f"  - è§†é¢‘æ–‡ä»¶: {video.filename}")
        
        # å¢å¼ºæ—¥å¿—ï¼Œè®°å½•æ‰€æœ‰æ”¶åˆ°çš„æ–‡æ¡£æ–‡ä»¶å
        if docs:
            file_names = [doc.filename for doc in docs if doc.filename]
            logger.info(f"  - å‚è€ƒæ–‡æ¡£: {len(file_names)} ä¸ª -> {file_names}")

        if not video or not video.filename:
            return jsonify({"error": "è¯·é€‰æ‹©è§†é¢‘æ–‡ä»¶"}), 400
        
        if ai_provider == 'openai' and not OPENAI_API_KEY:
            return jsonify({"error": "OpenAI API Keyæœªåœ¨æœåŠ¡å™¨ç«¯é…ç½®ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"}), 400
        
        if ai_provider == 'ollama':
            available_models = get_available_ollama_models()
            if not available_models:
                return jsonify({"error": "æœªæ£€æµ‹åˆ°å¯ç”¨çš„Ollamaæ¨¡å‹ï¼Œè¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ"}), 400
        
        # ä¿å­˜è§†é¢‘æ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶åå®‰å…¨
        video_filename = secure_filename(video.filename) if video and video.filename else f"video-{session_id}.mov"
        mov_path = os.path.join(app.config['UPLOAD_FOLDER'], video_filename)
        video.save(mov_path)
        
        # å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œç¡®ä¿æ–‡ä»¶åå”¯ä¸€
        doc_files = []
        for doc in docs:
            if doc and doc.filename:
                original_filename = secure_filename(doc.filename)
                # åˆ›å»ºå”¯ä¸€æ–‡ä»¶åæ¥é¿å…è¦†ç›–
                unique_filename = f"{session_id[:8]}-{uuid.uuid4().hex[:8]}-{original_filename}"
                saved_path = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
                doc.save(saved_path)
                doc_files.append({'original_filename': original_filename, 'saved_path': saved_path})
        
        # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­å¤„ç†
        thread = threading.Thread(
            target=process_meeting_async, 
            args=(session_id, mov_path, video_filename, doc_files, context_input, ai_provider, model)
        )
        thread.start()
        
        return jsonify({"session_id": session_id, "status": "processing"})
    except Exception as e:
        logger.error(f"âŒ å¤„ç†è¯·æ±‚é”™è¯¯: {str(e)}")
        return jsonify({"error": f"å¤„ç†è¯·æ±‚é”™è¯¯: {str(e)}"}), 500

def process_meeting_async(session_id: str, mov_path: str, video_filename: str, doc_files: list, context_input: str, ai_provider: str, model: str):
    """Asynchronously processes the meeting from video to summary."""
    progress = ProcessingProgress()
    progress.session_id = session_id
    logger.info(f"ğŸ¬ [{session_id[:8]}] Starting meeting processing...")
    start_time = time.time()
    
    # åŠ¨æ€é¢„ä¼°åˆå§‹æ—¶é•¿
    audio_duration_estimate = estimate_audio_duration(mov_path) # å…ˆå¯¹è§†é¢‘ä¼°ç®—
    transcription_estimate = audio_duration_estimate * 0.2 if audio_duration_estimate > 0 else 60 # ä¼°ç®—ä¸ºéŸ³é¢‘æ—¶é•¿çš„20%
    
    try:
        # å®šä¹‰å¤„ç†æ­¥éª¤å’Œé¢„ä¼°æ—¶é•¿(ç§’)
        progress.add_step("video_validation", "éªŒè¯è§†é¢‘æ–‡ä»¶", 2)
        progress.add_step("audio_extraction", "æå–éŸ³é¢‘", audio_duration_estimate * 0.05 + 5)
        progress.add_step("speech_transcription", "è¯­éŸ³è½¬æ–‡å­—", transcription_estimate)
        progress.add_step("document_processing", "å¤„ç†å‚è€ƒæ–‡æ¡£", 10)
        progress.add_step("image_analysis", "åˆ†ææ–‡æ¡£å›¾ç‰‡", 30)
        progress.add_step("ai_correction", "AIè½¬å½•çº é”™", 45)
        progress.add_step("summary_generation", "ç”Ÿæˆä¼šè®®çºªè¦", 35)
        progress.add_step("file_generation", "ç”Ÿæˆä¸‹è½½æ–‡ä»¶", 5)

        # Step 1: Validate Video
        progress.start_step(0)
        base_name = os.path.splitext(video_filename)[0]
        if not os.path.exists(mov_path):
            raise FileNotFoundError(f"Video file not found: {mov_path}")
        progress.complete_step(0)

        # Step 2: Extract Audio
        progress.start_step(1)
        audio_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_audio.wav')
        subprocess.run(['ffmpeg', '-i', mov_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', audio_path, '-y'], check=True, capture_output=True, text=True)
        audio_duration = estimate_audio_duration(audio_path)
        
        # æ›´æ–°è½¬å½•æ­¥éª¤çš„é¢„ä¼°æ—¶é•¿
        progress.steps[2]['estimated_duration'] = audio_duration * 0.2 
        progress.emit_progress() # é‡æ–°å¹¿æ’­ä¸€ä¸‹è¿›åº¦
        
        progress.complete_step(1)

        # Step 3: Transcribe Speech
        progress.start_step(2)
        # ä½¿ç”¨æ›´è¯¦ç»†çš„å‚æ•°è°ƒç”¨ï¼Œå¹¶å°è¯•å¯ç”¨è¯çº§æ—¶é—´æˆ³
        result = whisper_model.transcribe(audio_path, language=None, word_timestamps=True, verbose=False)
        transcript = str(result["text"])
        detected_lang = result.get("language", "unknown")
        progress.complete_step(2)

        # Step 4: Process Reference Documents
        progress.start_step(3)
        doc_texts = []
        pdf_files_for_image_analysis = []
        total_docs = len(doc_files)
        for i, doc_info in enumerate(doc_files):
            original_filename, saved_path = doc_info['original_filename'], doc_info['saved_path']
            progress.steps[3]['description'] = f"Processing {i+1}/{total_docs}: {original_filename}"
            progress.emit_progress()
            
            text = extract_text_from_file(saved_path)
            if text:
                doc_texts.append(f"## Reference: {original_filename}\n\n{text}")
            if original_filename.lower().endswith('.pdf'):
                pdf_files_for_image_analysis.append(saved_path)
        progress.complete_step(3)

        # Step 5: Analyze Images in PDFs
        progress.start_step(4)
        image_analysis_results = []
        if ai_provider == 'openai':
            for pdf_path in pdf_files_for_image_analysis:
                image_analysis_results.extend(analyze_pdf_images(pdf_path, ai_provider))
            if image_analysis_results:
                doc_texts.extend(image_analysis_results)
        progress.complete_step(4)

        # Step 6: AI Correction
        corrected_transcript = advanced_transcription_correction(transcript, context_input, doc_texts, progress, 5, ai_provider, model)
        
        # Step 7: Generate Summary
        meeting_summary = generate_meeting_summary(corrected_transcript, context_input, doc_texts, progress, 6, ai_provider, model)
        
        # Step 8: Generate Files
        progress.start_step(7)
        
        base_name = os.path.splitext(video_filename)[0]
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Generate raw transcript file
        raw_transcript_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_raw_transcript.md')
        with open(raw_transcript_path, 'w', encoding='utf-8') as f:
            f.write(f'# åŸå§‹è¯­éŸ³è½¬å½•\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {now_str}\n')
            f.write(f'**å¤„ç†è®¾å¤‡**: {whisper_device.upper()}\n')
            f.write(f'**æ£€æµ‹è¯­è¨€**: {detected_lang}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n\n')
            f.write(f'## è½¬å½•å†…å®¹\n\n{transcript}\n')

        # Generate corrected transcript file
        corrected_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_corrected_transcript.md')
        with open(corrected_path, 'w', encoding='utf-8') as f:
            f.write(f'# AIä¿®æ­£åçš„ä¼šè®®è½¬å½•\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {now_str}\n')
            f.write(f'**AIæ¨¡å‹**: {model}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n\n')
            f.write(f'## ä¿®æ­£åçš„è½¬å½•å†…å®¹\n\n{corrected_transcript}\n')

        # Generate full meeting report
        report_path = os.path.join(app.config['UPLOAD_FOLDER'], f'{base_name}_meeting_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f'# ä¼šè®®çºªè¦æŠ¥å‘Š\n\n')
            f.write(f'**ç”Ÿæˆæ—¶é—´**: {now_str}\n')
            f.write(f'**AIæ¨¡å‹**: {model}\n')
            f.write(f'**éŸ³é¢‘æ—¶é•¿**: {audio_duration:.1f}ç§’\n')
            f.write(f'**å‚è€ƒèµ„æ–™**: {len(doc_texts)} ä¸ª\n\n')
            
            if doc_texts:
                f.write(f'## å‚è€ƒèµ„æ–™æ¦‚è§ˆ\n\n')
                for i, doc in enumerate(doc_texts, 1):
                    first_line = doc.split('\n')[0] if doc else 'æœªçŸ¥æ–‡æ¡£'
                    f.write(f'{i}. {first_line.replace("##", "").strip()}\n')
                f.write(f'\n---\n\n')
            
            f.write(f'{meeting_summary["summary"]}\n\n')
            f.write(f'---\n\n## é™„å½•ï¼šAIä¿®æ­£åå®Œæ•´è®°å½•\n\n{corrected_transcript}\n')

        progress.complete_step(7)

        # Final step: Notify client
        total_time = time.time() - start_time
        logger.info(f"ğŸ‰ [{session_id[:8]}] Processing complete! Total time: {total_time:.1f}s")
        
        socketio.emit('processing_complete', {
            'raw_transcript_path': raw_transcript_path,
            'corrected_transcript_path': corrected_path,
            'report_path': report_path,
            'processing_time': total_time,
            'audio_duration': audio_duration,
            'detected_language': detected_lang,
            'ai_provider': ai_provider,
            'model': model
        }, to=session_id)

    except Exception as e:
        logger.error(f"ğŸ’¥ [{session_id[:8]}] Processing error: {e}", exc_info=True)
        socketio.emit('error', {'message': f'An error occurred: {str(e)}'}, to=session_id)

@app.route('/download/<filename>')
def download_file(filename):
    """ä¸‹è½½ç”Ÿæˆçš„æ–‡ä»¶"""
    try:
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if os.path.exists(file_path):
            return send_file(file_path, as_attachment=True)
        else:
            return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸‹è½½é”™è¯¯: {e}")
        return jsonify({"error": f"æ–‡ä»¶ä¸‹è½½é”™è¯¯: {str(e)}"}), 500

def analyze_pdf_images(file_path: str, ai_provider: str) -> List[str]:
    """ä»PDFä¸­æå–å›¾ç‰‡å¹¶ä½¿ç”¨AIè¿›è¡Œåˆ†æ"""
    if ai_provider != 'openai' or not OPENAI_API_KEY:
        logger.info("éOpenAIæä¾›å•†æˆ–æœªé…ç½®Keyï¼Œè·³è¿‡å›¾ç‰‡åˆ†æ")
        return []

    logger.info(f"ğŸ–¼ï¸ å¼€å§‹ä»PDFä¸­æå–å’Œåˆ†æå›¾ç‰‡: {os.path.basename(file_path)}")
    image_analyses = []
    try:
        doc = fitz.open(file_path)
        for page_num in range(len(doc)):
            image_list = doc.get_page_images(page_num)
            for img_index, img in enumerate(image_list, 1):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # å°†å›¾ç‰‡è½¬æ¢ä¸ºå¯å‘é€çš„æ ¼å¼
                image = Image.open(BytesIO(image_bytes))
                
                # ç¡®ä¿æ˜¯RGBæ ¼å¼
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                    
                buffered = BytesIO()
                image.save(buffered, format="JPEG")
                img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                
                # AIåˆ†æå›¾ç‰‡
                prompt = "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æœå®ƒæ˜¯ä¸€ä¸ªå›¾è¡¨ï¼Œè¯·è§£è¯»å…¶æ•°æ®ã€è¶‹åŠ¿å’Œå…³é”®ä¿¡æ¯ã€‚å¦‚æœæ˜¯ä¸€ä¸ªæµç¨‹å›¾ï¼Œè¯·è§£é‡Šå…¶æ­¥éª¤å’Œé€»è¾‘ã€‚å¦‚æœæ˜¯ä¸€å¼ æˆªå›¾ï¼Œè¯·æè¿°å…¶ç•Œé¢å’ŒåŠŸèƒ½ã€‚"
                analysis = call_openai_api(prompt, image_base64=img_base64, max_tokens=500)
                
                if analysis:
                    analysis_text = f"## æ¥è‡ªPDF '{os.path.basename(file_path)}' (ç¬¬{page_num+1}é¡µ, å›¾{img_index})çš„å›¾è¡¨åˆ†æ\n\n{analysis}\n"
                    image_analyses.append(analysis_text)
                    logger.info(f"âœ… æˆåŠŸåˆ†æäº†PDF '{os.path.basename(file_path)}' ä¸­çš„ä¸€å¼ å›¾ç‰‡")
                else:
                    logger.warning(f"âš ï¸ æœªèƒ½åˆ†æPDF '{os.path.basename(file_path)}' ä¸­çš„ä¸€å¼ å›¾ç‰‡")

        doc.close()
    except Exception as e:
        logger.error(f"âŒ åˆ†æPDFå›¾ç‰‡æ—¶å‡ºé”™: {e}")
    
    return image_analyses

# ä»æ–‡ä»¶åŠ è½½æç¤ºè¯
def load_prompt(filename: str) -> str:
    """ä»æ–‡ä»¶åŠ è½½æç¤ºè¯"""
    try:
        prompt_path = os.path.join('prompts', filename)
        if not os.path.exists(prompt_path):
            logger.error(f"âŒ æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
            return ""
            
        with open(prompt_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            
        if not content:
            logger.error(f"âŒ æç¤ºè¯æ–‡ä»¶ä¸ºç©º: {filename}")
            return ""
            
        if len(content) < 50:  # æç¤ºè¯å¤ªçŸ­å¯èƒ½æœ‰é—®é¢˜
            logger.warning(f"âš ï¸ æç¤ºè¯æ–‡ä»¶è¿‡çŸ­: {filename} ({len(content)} å­—ç¬¦)")
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å ä½ç¬¦
        if filename == 'correction_prompt.txt':
            required_placeholders = ['{{transcript}}', '{{context}}', '{{reference_docs}}']
        elif filename == 'summary_prompt.txt':
            required_placeholders = ['{{corrected_transcript}}', '{{context}}', '{{reference_docs}}']
        else:
            required_placeholders = []
            
        missing_placeholders = []
        for placeholder in required_placeholders:
            if placeholder not in content:
                missing_placeholders.append(placeholder)
        
        if missing_placeholders:
            logger.error(f"âŒ æç¤ºè¯æ–‡ä»¶ç¼ºå°‘å¿…è¦å ä½ç¬¦: {filename} - ç¼ºå°‘: {missing_placeholders}")
            return ""
            
        logger.info(f"âœ… æç¤ºè¯åŠ è½½æˆåŠŸ: {filename} ({len(content)} å­—ç¬¦)")
        return content
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æç¤ºè¯æ–‡ä»¶å¤±è´¥ {filename}: {str(e)}")
        return ""

# åŠ è½½æç¤ºè¯æ¨¡æ¿
correction_prompt_template = load_prompt('correction_prompt.txt')
summary_prompt_template = load_prompt('summary_prompt.txt')

# éªŒè¯æç¤ºè¯æ˜¯å¦åŠ è½½æˆåŠŸ
if not correction_prompt_template:
    logger.error("âŒ è½¬å½•çº é”™æç¤ºè¯åŠ è½½å¤±è´¥ï¼ŒAIçº é”™åŠŸèƒ½å°†ä¸å¯ç”¨")
if not summary_prompt_template:
    logger.error("âŒ çºªè¦ç”Ÿæˆæç¤ºè¯åŠ è½½å¤±è´¥ï¼ŒAIçºªè¦åŠŸèƒ½å°†ä¸å¯ç”¨")

if __name__ == '__main__':
    # å¯åŠ¨æ—¶æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    logger.info("=" * 50)
    logger.info("ğŸ¤ æ™ºèƒ½ä¼šè®®çºªè¦ç”Ÿæˆå™¨å¯åŠ¨")
    logger.info(f"ğŸ–¥ï¸  Whisperè®¾å¤‡: {whisper_device.upper()}")
    logger.info(f"ğŸ¤– OpenAIé…ç½®: {'âœ… å·²é…ç½®' if OPENAI_API_KEY else 'âŒ æœªé…ç½®'}")
    
    # æ£€æŸ¥Ollama
    ollama_models = get_available_ollama_models()
    logger.info(f"ğŸ¦™ Ollamaæ¨¡å‹: {len(ollama_models)} ä¸ªå¯ç”¨")
    for model in ollama_models:
        logger.info(f"   - {model['name']}")
    
    # æ£€æŸ¥æç¤ºè¯æ–‡ä»¶
    logger.info("=" * 50)
    logger.info("ğŸ“ åŠ è½½AIæç¤ºè¯:")
    logger.info(f"   - è½¬å½•ä¿®æ­£: {'âœ… å·²åŠ è½½' if correction_prompt_template else 'âŒ åŠ è½½å¤±è´¥'}")
    logger.info(f"   - çºªè¦ç”Ÿæˆ: {'âœ… å·²åŠ è½½' if summary_prompt_template else 'âŒ åŠ è½½å¤±è´¥'}")
    logger.info("=" * 50)
    
    socketio.run(app, debug=True, port=5000)